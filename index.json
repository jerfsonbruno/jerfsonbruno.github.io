[{"categories":null,"content":"Teste lldflpdf ","date":"17-12-2022","objectID":"/sobre/sobremim/:0:0","tags":null,"title":"Sobremim","uri":"/sobre/sobremim/"},{"categories":null,"content":"Este trabalho foi apresentado no CNMAC e Conapesc 2022, e teve orientação da Prof. Dr. Getúlio José Amorim Do Amaral, gjaa@de.ufpe.br. Em diversas ocasiões, em análise estatística de forma, é necessário agrupar um conjunto de dados em grupos, de tal maneira, que se tenha grupos com características mais homogêneas. Com isso, ter algoritmos que trabalhem no espaço não euclidiano, como é no caso dos dados de forma ou tamanho e forma, é necessário. ","date":"06-07-2022","objectID":"/oitavo-post/:0:0","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Classificação Não Supervisionada Algoritmos de agrupamento por particionamento encontram uma partição que maximiza ou minimiza algum critério numérico. Este trabalho tenta adaptar para os dados de tamanho e forma, o algoritmo $K$-médias proposto por . Uma descrição do algoritmo $K$-médias introduzido por , usa uma partição inicial e depois o valor médio dos objetos em um grupo como o protótipo de partição. Ao trabalharmos com dados de tamanho e forma, a distância, normalmente Euclidiana, é substituída pela distância de procrustes no espaço de tamanho e forma, e ela é dada pela Equação abaixo: $$ d_S (Y_1, Y_2) = \\sqrt{S_1^2 + S_2^2 - 2S_1S_2 \\cos \\rho(Y_1,Y_2)}$$ em que $S_1$, $S_2$ são os tamanhos dos centroides de $Y_1$, $Y_2$, e $\\rho$ é a distância Riemannian. Para mais detalhes veja minha dissertação. A medida de dissimilaridade utilizada no $K$-médias, é a soma dos quadrados totais. A mesma modificada para os dados de tamanho e forma é definida como: $$ J= \\sum_{m=1}^{\\pi_m} \\sum_{n=1}^{n_m} d_S(Y_{mn},\\mu_{m})$$ em que $Y_{mn}$ é o $n$-ésima observação do grupo $m$ e $\\mu$ é a média do grupo. Com isso, o algoritmo $K$-médias para tamanho e forma tem os seguintes passos: $K$-means modificado para dados de tamanho e forma Passo 1: Dados de Entrada 1.1 Seja $\\Omega = 1,…, n$ um conjunto de dados descrito pela matriz de configuração $\\mathbf{Y}$. 1.2 Calcule os dados de tamanho e forma $\\mathbf{Y_C}$. 1.3 Escolha $K$ centroides e gere uma partição inicial $\\pi_K$ 1.4 Fixe o número de iterações $T=100$ e um erro $\\varepsilon \u003e 0$; Faça $t = 1$. 1.5 Calcule as médias $\\mu_{K_t}$ dos grupos $\\pi_{K_t}$. Passo 2: Atribuindo os objetos aos grupos 2.1 Calcule a distância de cada objeto em relação a média $\\mu_{K_t}$ e atribua esse objeto ao novo grupo $\\pi_{K_{t+1}}$ mais próximo. 2.1 Calcule as médias dos novos grupos $\\pi_{K_{t+1}}$ formados. Passo 3: Critério de Parada 3.1 Se |$\\pi_{K_{t+1}} - \\pi_{K_{t}}|\u003c \\varepsilon$ ou $t\u003eT$, então Pare. Caso contrário, faça $t=t+1$ e volte para o passo $2$. ","date":"06-07-2022","objectID":"/oitavo-post/:1:0","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Validação do Cluster A validade de um estudo refere-se a quão bem os resultados encontrados representam resultados verdadeiros para indivíduos semelhantes fora do estudo. Este conceito de validade se aplica a todos os tipos de estudos, seja ele clínicos, sobre prevalência, associações, intervenções e diagnóstico. A validade de um estudo de pesquisa inclui dois domínios: a validade interna e a validade externa. ","date":"06-07-2022","objectID":"/oitavo-post/:2:0","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Índice Interno A validade interna é definida como a extensão em que os resultados observados representam uma possível verdade para a população. Em nosso trabalho, usaremos o índice residual de Procrustes, já que o mesmo é muito utilizado em análise estatística de forma. Para mais detalhes veja minha dissertação. O Índice Residual Procrustes é útil para encontrar o número ideal de grupos, e também avaliar a qualidade do ajuste em um conjunto de dados em análise estatística de formas. Após obter a alocação dos indivíduos do conjunto de dados pelo o algoritmo $K$-médias. Calcula-se a norma quadrática dos resíduos de cada indivíduo dentro do seu grupo $(r_{in})$ e fora do seu grupo $(r_{out})$. Com isso, o índice residual Procrustes $pr(i)$ para cada indivíduo do conjunto de dados é dado como $$pr(i) = \\frac{r_{out} (i) -r_{int} (i)}{\\max(r_{out} (i),r_{int} (i))}$$ em que $-1 \\leq pr(i) \\leq 1$. Valores próximos de $1$ indicam que o indivíduo $i$ possui dissimilaridade menor dentro do grupo comparando com outro grupo, logo o indivíduo está agrupado apropriadamente. Valores negativos ou próximos de $-1$ indicam que o indivíduo $i$ pode ter sido alocado no grupo errado. Para se obter um índice de validação geral de Procrutes, é feito a média de todos os $pr(i)$: $$PR = \\frac{1}{n} \\sum_{i=1}^{n} pr(i)$$ Uma Tabela de interpretação para os resultados do $PR$, é baseada na tabela de interpretação do índice silhueta, que pode ser visto Aqui. ","date":"06-07-2022","objectID":"/oitavo-post/:2:1","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Índice Externo Feito por , o índice Rand ou medida Rand em estatística, é uma medida da similaridade entre agrupamentos de dados. Do ponto de vista matemático, o índice de Rand está relacionado à precisão, mas é aplicável mesmo quando os rótulos de classe não são usados. Denotado por $R$, o índice Rand é calculado como: $$ R=\\frac{a+b}{C_{n,2}}$$ em que, $a$, é o número de vezes que um par de elementos pertence ao mesmo cluster; $b$ é o número de vezes que um par de elementos pertence a grupos de diferentes; e $C_{n,2}$ é número de pares não ordenados em um conjunto de $n$ elementos. Valores próximos de $1$ indicam que o agrupamento está bem definido, logo foi agrupado apropriadamente. Valores próximos de $0$ indicam que o agrupamento pode ter sido definido de forma errada. ","date":"06-07-2022","objectID":"/oitavo-post/:2:2","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Acurácia Quando se fala em tecnologia atualmente, um termo que tem se tornado cada vez comum é o da acurácia. Usada muitas vezes quase como um sinônimo de precisão e eficiência. Em outras palavras, a acurácia serve para ver a porcentagem de acertos do algoritmo de agrupamento. ","date":"06-07-2022","objectID":"/oitavo-post/:2:3","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Aplicação e Resultados Para verificar o comportamento do algoritmo proposto, foram criados dados artificiais de tamanho e forma contendo dois grupos. Nesses dados simulados, gerados a partir da distribuição normal complexa central, propomos dois cenários possíveis: O primeiro para grupos homogêneos e o segundo para dois grupos heterogêneo. ","date":"06-07-2022","objectID":"/oitavo-post/:3:0","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Cenário 1: grupos homogêneos. No primeiro cenário, ao construirmos o tamanho dos centroides de cada grupo, podemos ver o quanto eles são semelhantes. Neste cenário, o algoritmo teve um pouco de dificuldade para realizar o agrupamento. Porém, seus índices interno e externos, além da acurácia, comprovam que o algoritmo teve boa performance. Índice de Rand Índice de Procrustes Acurácia $K$-médias 0,809 0,495 89,58% ","date":"06-07-2022","objectID":"/oitavo-post/:3:1","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Cenário 2: grupos heterogêneo. No segundo cenário, ao construirmos o tamanho dos centroides de cada grupo, podemos ver que os grupo são bastantes diferentes. Nesse cenário, o algoritmo teve melhor desempenho se comparado ao primeiro cenário. Índice de Rand Índice de Procrustes Acurácia $K$-médias 0,958 0,627 97,92% Com isso, concluímos que à medida que o tamanho dos centroides dos grupos diferem, o algoritmo vai tendo mais facilidade em realizar o agrupamento. Além disso, provamos que o algoritmo é eficiente para tratar dados de tamanho e forma. ","date":"06-07-2022","objectID":"/oitavo-post/:3:2","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Dados Reais Após a validação com os dados artificiais, aplicaremos o algoritmo $K$-médias modificado para duas bases de dados reais. As mesmas podem ser encontradas no pacote shapes do software ","date":"06-07-2022","objectID":"/oitavo-post/:4:0","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Vértebras torácicas T2 de camundongos. A base de dados das vértebras torácicas T2 de camundongos possui 6 marcos em 2 dimensões, e possui um total de 76 indivíduos. Esses indivíduos foram classificados em 3 grupos: controle(c)=30, grandes(l)=23 e pequenos(s)=23. Os 6 pontos de referência foram obtidos usando um método semi-automático em pontos de alta curvatura, em que, é mostrado com mais detalhes aqui. Para ter uma ideia inicial de como os algoritmos se comportarão, foi feito o boxplot do tamanho dos centroides de cada grupo. Pela Figura abaixo, podemos ver que apenas um grupo difere dos demais, além da presença de outliers em dois grupos. Com isso, podemos supor a partir dos resultados dos dados simulados, que o algoritmo terá uma boa performance para realizar os agrupamentos. A Tabela abaixo mostra os resultados do algoritmo, com ela, podemos ver que os resultados a performance de cada índice. De modo geral, os índices interno e externo de validação tiveram bons resultados, indicando assim que os agrupamentos foram bem definidos e posteriormente, validando os agrupamentos. Índice de Rand Índice de Procrustes Acurácia $K$-médias 0,737 0,592 75,0% ","date":"06-07-2022","objectID":"/oitavo-post/:4:1","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Ressonância magnética de pessoas com esquizofrenia A esquizofrenia é um transtorno mental grave que muda o modo como a pessoa pensa, sente e se comporta socialmente. A base de dados das ressonâncias magnéticas de pessoas com esquizofrenia possui 13 marcos em 2 dimensões, e possui um total de 28 indivíduos. Esses indivíduos foram classificados em 2 grupos: controle(con)=14 e esquizofrênico(scz)=14. Para mais detalhes sobre a base de dados, ver . Novamente, para ter uma ideia inicial de como os algoritmos se comportarão, foi feito o boxplot do tamanho dos centroides de cada grupo. Pela Figura abaixo, podemos ver que os dois grupos estão bem separados, porém, existe a presença de outliers nos dois grupos. Com isso, podemos supor a partir dos resultados dos dados simulados, que os algoritmos terão uma boa performance para realizar os agrupamentos. A Tabela abaixo mostra os resultados do algoritmo, com ela, novamente, podemos ver que os resultados e a performance de cada índice. Os índices interno e externo de validação tiveram bons resultados, indicando assim que os agrupamentos foram bem definidos e posteriormente, validando os agrupamentos. Índice de Rand Índice de Procrustes Acurácia $K$-médias 0,696 0,419 82,1% ","date":"06-07-2022","objectID":"/oitavo-post/:5:0","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Conclusões Neste trabalho, apresentamos uma introdução a análise estatística de formas e classificação não supervisionada no espaço não euclidiano. No geral, os resultados obtidos mostraram que o algoritmo proposto obteve bom desempenho, para realizar os agrupamentos, usando dados de tamanho e forma. Se tratando dos dados simulados e reais, o algoritmo foi eficiente para todos os cenários propostos. Para o cenário em que os os grupos eram mais homogêneos, o algoritmo perdeu um pouco de eficiência, para grupos mais heterogêneos o algoritmo foi mais eficiente. Assim, este trabalho contribuiu para a literatura teórica dos métodos de agrupamento para dados de análise estatística de tamanho e forma. ","date":"06-07-2022","objectID":"/oitavo-post/:6:0","tags":null,"title":"Classificação não supervisionada no contexto não euclidiano. ","uri":"/oitavo-post/"},{"categories":null,"content":"Este trabalho foi apresentado no CNMAC e Conapesc 2022, e teve orientação da Prof. Dr. Getúlio José Amorim Do Amaral, gjaa@de.ufpe.br. O objetivo deste artigo é dá uma introdução sobre análise estatística de forma e posteriormente propor um método de classificação não supervisionado ($K$-médias) para dados de tamanho e forma considerando imagens bidimensionais (formas planas). ","date":"06-07-2022","objectID":"/setimo-post/:0:0","tags":null,"title":"Uma Introdução a análise estatística de forma","uri":"/setimo-post/"},{"categories":null,"content":"Introdução Com os avanços da tecnologia, a captura de imagens bidimensionais e tridimensionais tem se tornado cada vez mais comum no nosso cotidiano. Essas imagens fornecem diversas informações para estudos estatísticos, sendo essa área chamada de morfometria. A morfometria é uma das maneiras de estudar estas imagens que se encontram bem consolidadas com diversas aplicações, tais como: Medicina, Zoologia, Biologia e outros. Nesse contexto, existem estudos que tratam da forma, e estudos que tratam o tamanho e forma, dos objetos capturados nas imagens. No caso de forma, os efeitos de locação, escala e rotação são removidos. No caso de tamanho e forma, o efeito de escala não é removido. No século atual, o amadurecimento da análise estatística de forma como uma área teórica e aplicada vem crescendo, uma vez que no atual século a maioria das tecnologias usam reconhecimento facial, ou seja, propriedades geométricas de tamanho e forma. As aplicações da análise estatística de forma se estendem por quase todas as áreas científicas e tecnológicas aplicadas, das menores às maiores escalas. A análise de de forma ou tamanho e forma dos objetos, pode ser útil para a tomada de importantes decisões, como a de um médico que precisa decidir se um câncer é maligno ou benigno, baseado em uma ressonância magnética digitalizada. ","date":"06-07-2022","objectID":"/setimo-post/:1:0","tags":null,"title":"Uma Introdução a análise estatística de forma","uri":"/setimo-post/"},{"categories":null,"content":"Desenvolvimento ","date":"06-07-2022","objectID":"/setimo-post/:2:0","tags":null,"title":"Uma Introdução a análise estatística de forma","uri":"/setimo-post/"},{"categories":null,"content":"Análise Estatística de Forma Formas de objetos estão disponíveis em todos os lugares, seja em uma pesquisa na internet, uma ressonância magnética que você faz, ou até mesmo no desbloqueio de um celular. Essas tomadas de decisões servem para pesquisar, identificar, classificar e agrupar informações. A análise estatística de formas é um tópico relativamente recente e está relacionada com características e comparações de formas de objetos. descrevem com mais detalhes os conceitos sobre análise de formas em seu livro. Uma maneira de descrever uma forma é indicar um subconjunto finito de pontos no contorno do objeto. O número de pontos não seguem uma restrição teórica segundo . Esse número finito de pontos de cada objeto é conhecido como marcos. Segundo existe três tipos marcos, são eles: Marcos matemáticos: são pontos alocados em um objeto de acordo com propriedades matemáticas ou geométricas, por exemplo, pontos de máximo, mínimo, inflexão entre outros; Marcos anatômicos: são pontos atribuídos por um especialista que correspondem a alguma característica biologicamente significativa; Pseudo-Marcos: são pontos construídos em um objeto, localizados no contorno ou entre os marcos anatômicos e/ou matemáticos. As Figuras mostram um exemplo da utilização desses marcos: A partir da colocação dos marcos, e obtendo suas coordenadas, podemos construir a matriz de configuração, e assim obter a forma do objeto. Para isso, devemos realizar transformações matemáticas para remover efeitos de escala, locação e rotação. Pois, formas são definidas como toda informação geométrica que permanece quando os efeitos de locação, escala e rotação são retirados de um objeto. Para ver como é feita a retirada desses efeitos, veja minha dissertação. A partir da remoção desse efeito, é onde entramos nos três tipos de configurações mais usados; são eles: Pré-Forma, Forma e Tamanho-e-forma. A Figura abaixo representa em diagrama essas três configurações, e como elas se relacionam entre sí: ","date":"06-07-2022","objectID":"/setimo-post/:2:1","tags":null,"title":"Uma Introdução a análise estatística de forma","uri":"/setimo-post/"},{"categories":null,"content":"Esse trabalho foi apresentado na disciplina de Estatística aplicada ministrada pelo Prof. Dr. Francisco Cribari Neto. E teve como inspiração, um trabalho feito pelo Prof. Dr. Marcus Alexandre Nunes. O professor Marcus Nunes tem um blog (https://marcusnunes.me), recomendo fortemente que vocês acessem. Não deve ser surpresa para ninguém as reportagens sobre prováveis subnotificações a respeito dos casos de COVID-19. A fim de verificar se essas suspeitas são mesmo procedentes, decidi comparar os números de casos de Síndrome Respiratória Aguda Grave (SRAG) registrados no Brasil e em Pernambuco nos últimos 10 anos, a fim de tentar detectar se houve um aumento nos casos em 2020/2021. Com a análise inicial, já foi possível ver tanto no Brasil como em Pernambuco um aumento extraordinário no ano de 2020/2021. Foi feito também uma análise sobre a média de casos em relação ao ano de 2020, vendo assim um valor alarmante do ano de 2020. Assim, temos altos indícios de que existe subnotificação do COVID-19. Fizemos dois métodos de previsão, o primeiro usando o algoritmo de Holt-Winters e o segundo usando o modelo ARIMA. Fizemos uma previsão para as próximas seis semanas e comparamos com o valor real. Tivemos um erro menor no modelo ARIMA, indicando assim como um modelo mais preciso para a série SRAG. ","date":"01-03-2021","objectID":"/dez-post/:0:0","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Introdução Não deve ser surpresa para ninguém as reportagens sobre prováveis subnotificações a respeito dos casos de COVID-19. A fim de verificar se essas suspeitas são mesmo procedentes, decidi comparar os números de casos de Síndrome Respiratória Aguda Grave (SRAG) registrados no Brasil e em Pernambuco nos últimos 10 anos, a fim de tentar detectar se houve um aumento nos casos em 2020/2021. Como sabemos, infelizmente, não há testes de COVID-19 para todos os suspeitos. A Síndrome Respiratória Aguda Grave (SRAG) possui sintomas muito parecidos com aqueles do COVID-19. Com isso, acredito que, com o avanço da pandemia, é importante que a população tenha noção da quantidade de casos da doença que estão por aí. Temos um bom acesso a casos registrados de SRAG no site da Fiocruz. Eles desenvolveram uma excelente ferramenta de divulgação chamada , no qual obtive os dados aqui divulgados. Com isso, faremos uma análise para a série temporal, para tentar identificar se os modelos usuais servem para prever se a um aumento ou decrescimento nos futuros casos registrados de SRAG. ","date":"01-03-2021","objectID":"/dez-post/:1:0","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Revisão da Literatura ","date":"01-03-2021","objectID":"/dez-post/:2:0","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"SRAG Segundo o , existe duas variações, sendo elas: Síndrome Gripal (SG): Indivíduo com quadro respiratório agudo, caracterizado por pelo menos dois (2) dos seguintes sinais e sintomas: febre (mesmo que referida), calafrios, dor de garganta, dor de cabeça, tosse, coriza, distúrbios olfativos ou distúrbios gustativos. Síndrome Respiratória Aguda Grave (SRAG): Indivíduo com SG que apresente: dispneia/desconforto respiratório ou pressão persistente no tórax ou saturação de O2 menor que 95% em ar ambiente ou coloração azulada dos lábios ou rosto. Em casos de COVID-19, existem três critérios parar confirmar a doença: Por Critério Clínico: Caso de SG ou SRAG com confirmação clínica associado a anosmia (disfunção olfativa) ou ageusia (disfunção gustatória) aguda sem outra causa pregressa. Por Critério Clínico-Epidemiológico: Caso de SG ou SRAG com histórico de contato próximo ou domiciliar, nos 14 dias anteriores ao aparecimento dos sinais e sintomas com caso confirmado para COVID-19. Por Critério Clínico-Imagem: Caso de SG ou SRAG ou óbito por SRAG que não foi possível confirmar por critério laboratorial e que apresente algumas alterações tomografias. Com essas informações, fica claro que pode haver casos de subnotificações do COVID-19 nos dados do SRAG. ","date":"01-03-2021","objectID":"/dez-post/:2:1","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Holt-Winters O método de alisamento exponencial Holt-Winters, ou simplesmente algoritmo Holt-Winters, compreende a equação de previsão e três equações de suavização Uma para o nível $l_t$, Uma para a tendência $b_t$, E uma para a componente sazonal $s_t$. com os parâmetros de suavização $\\alpha$, $\\beta$ e $\\gamma$ . Existem duas variações desse método que diferem na natureza do componente sazonal. O método aditivo é preferido quando as variações sazonais são aproximadamente constantes ao longo da série, enquanto o método multiplicativo é preferido quando as variações sazonais estão mudando proporcionalmente ao nível da série. Com o método aditivo, a componente sazonal é expressa em termos absolutos na escala das séries observadas e na equação de nível a série é ajustada sazonalmente subtraindo a componente sazonal. Dentro de cada ano, o componente sazonal somará aproximadamente zero. Com o método multiplicativo, a componente sazonal é expressa em termos relativos (percentagens) e a série é ajustada sazonalmente pela divisão pela componente sazonal. As Equações abaixo correspondem aos métodos aditivo e multiplicativo respectivamente: Aditivo $$\\text{Equação de Nivel: }\\hspace{0.5cm} l_t = \\alpha(y_t -s_{t-m}) + (1-\\alpha) (l_{t-1} + b_{t-1})$$ $$\\text{Equação de Tendência: }\\hspace{0.5cm} b_t = \\beta^* (l_t - l_{t-1}) + (1-\\beta^*)b_{t-1}$$ $$\\text{Equação de Sazonalidade: }\\hspace{0.5cm} s_t = \\gamma (y_t - l_{t-1}-b_{t-1}) + (1-\\gamma)s_{t-m}$$ Multiplicativo $$\\text{Equação de Nivel: } \\hspace{0.5cm}l_t = \\alpha(\\frac{y_t}{s_{t-m}} ) + (1-\\alpha) (l_{t-1} + b_{t-1})$$ $$\\text{Equação de Tendência: }\\hspace{0.5cm} b_t = \\beta^* (l_t - l_{t-1}) + (1-\\beta^*)b_{t-1}$$ $$\\text{Equação de Sazonalidade: }\\hspace{0.5cm} s_t = \\gamma (\\frac{y_t}{l_{t-1}-b_{t-1}} ) + (1-\\gamma)s_{t-m}$$ A equação de previsão é dado por $$ \\text{Equação de previsão:} \\hspace{0.5cm} \\hat{y}_{t+h|t} = l_t +hb_t + s_w$$ em que $w=t+h-m(k+1)$ ","date":"01-03-2021","objectID":"/dez-post/:2:2","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"ARIMA Os modelos ARIMA fornecem outra abordagem para a previsão de séries temporais. A suavização exponencial e os modelos ARIMA são as duas abordagens mais amplamente utilizadas para a previsão de séries temporais e fornecem abordagens complementares para o problema. Enquanto os modelos de suavização exponencial são baseados em uma descrição da tendência e sazonalidade dos dados, os modelos ARIMA visam descrever as autocorrelações nos dados. Em um modelo de regressão múltipla, prevemos a variável de interesse usando uma combinação linear de preditores. Em um modelo de autorregressão, prevemos a variável de interesse usando uma combinação linear de valores anteriores da variável. O termo autorregressão indica que é uma regressão da variável contra si mesma. Define-se um modelo autorregressivo de ordem $p$, $AR(p)$, como: $$ y_t = c+\\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} + \\varepsilon_t$$ em que $\\varepsilon_t$ é um ruído branco com distribuição $N(0,\\sigma^2)$. Isso é como uma regressão múltipla, mas com valores defasados de $y_t$ como preditores. Também é possível em vez de usar valores anteriores da variável de previsão em uma regressão, usar um modelo de média móvel para os erros de previsão. Chamamos esse método como modelo de médias móveis, dado pela equação: $$y_t = c+\\varepsilon_t + \\theta_1 y_{t-1} +\\theta_2 \\varepsilon_{t-2} + \\cdots + \\theta_p \\varepsilon_{t-p}, $$ onde $\\varepsilon_t$ é um ruído branco. Referimos a isso como um modelo de média móvel de ordem $q$, $MA(q)$. Se combinarmos com autorregressão e um modelo de média móvel, obtemos um modelo ARMA. O modelo ARMA, pode ser estendido para o modelo ARIMA, em que ARIMA é um acrônimo para AutoRegressive Integrated Moving Average (neste contexto, “integração” é o reverso de diferenciação). O modelo ARIMA$(p, q, d)$, tem as seguintes nomenclaturas: $$ p= \\text{Ordem da parte autoregressiva}$$ $$d= \\text{Números de diferenças feitas}$$ $$q= \\text{Ordem da parte de médias móveis}$$ Percebe-se que, Se $p\\neq0$ e $q=0$, temos um modelo $AR(p)$, Se $q\\neq0$ e $p=0$, temos um modelo $MA(q)$, A ordem $d$, depende se a série for estacionária ou não. ","date":"01-03-2021","objectID":"/dez-post/:2:3","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Aplicação Os dados analisados foram coletados pelo . Os dados são semanais, do ano de 2011 até 2021. Faremos uma análise descritiva, e em seguida ajustaremos os métodos de previsão de séries temporais usuais e os compararemos. Para visualizar e comparar um potencial modelo, separamos os dados em treinamento e teste. O treinamento são dados da primeira semana de 2011 até a quadragésima nona a semana do ano de 2020. Os modelos serão construídos com base nos dados de treinamento. A base de testes é da quadragésima nona semana de 2020 até a terceira semana de 2021. A mesma serve para comparar o valor verdadeiro com o valor predito. Lembrando que analisaremos duas séries temporais, a série SRAG do Brasil e a série SRAG do estado de Pernambuco. ","date":"01-03-2021","objectID":"/dez-post/:3:0","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Análise Descritiva Podemos ver pelas Tabelas (1) e (2) algumas estatísticas sobre as séries temporais. Na Tabela (1) podemos ver que o valor mínimo de casos ocorridos em uma semana no Brasil foi $14$, e que como a média se difere bastante da mediana e o terceiro quartil, observando então, um grande aumento no número de casos nas semanas finais dos dados. O valor máximo de casos em uma semana foi de $17943$. Tabela 1 - Algumas Estatísticas para a série SRAG Brasil. Min 1Q Mediana Média 3Q Máx 14 151,2 313,5 1270,5 710 17943 Na Tabela (2) podemos ver também que como a média se difere bastante da mediana e do terceiro quartil, tendo novamente, um grande aumento no número de casos nas ultimas semanas. O valor máximo de casos em uma semana foi de $1312$. Tabela 2 - Algumas Estatísticas para a série SRAG Pernambuco. Min 1Q Mediana Média 3Q Máx 0 9 22 64,66 42 1312 A partir dessa primeira análise, foi feito um gráfico mostrando o número de casos ao longo das semanas, filtrados pelos anos. Com isso, podemos ver os primeiros indícios de subnotificação do COVID-19. Tanto na Figura (1) envolvendo todo o Brasil, como na Figura (2) envolvendo todo o estado de Pernambuco, o ano de 2020 está claramente destacado por conter uma curva muito superior a todos os outros anos. O segundo destaque é o ano de 2016. Em 2016 rolou uma grande epidemia de Influenza A (H1N1), com isso, o aumento da curva de 2016 pode ser justificada. Claramente está ocorrendo algum “fenômeno” que fez as notificações de SRAG aumentarem no ano de 2020. Com isso, temos indícios de subnotificação do COVID-19. Figura 1 – Número de Casos ao Longo das Semanas Filtrado Pelos Anos Para a Série SRAG Brasil. Figura 2 – Número de Casos ao Longo das Semanas Filtrado Pelos Anos Para a Série SRAG Pernambuco. Para melhorar, ou dar mais evidências das subnotificações, foi construído um gráfico para comparar o ano de 2020 com a média histórica dos anos de 2011 a 2019. A partir disso, podemos observar nas Figuras (3) e (4) uma diferença absurdas entres as curvas. Figura 3 – Número de Casos ao Longo das Semanas do ano 2020 em Comparação Com Média dos Anos de 2011 a 2019 para a Série SRAG Brasil. Figura 4 – Número de Casos ao Longo das Semanas do ano 2020 em Comparação Com Média dos Anos de 2011 a 2019 para a Série SRAG Pernambuco. Finalizado essa breve análise, partiremos para ver o comportamento das previsões dos métodos usuais de séries temporais com a série SRAG (Brasil e Pernambuco). ","date":"01-03-2021","objectID":"/dez-post/:3:1","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Série Temporal Para tentar fazer previsões com o máximo de fidelidade ao valor real, tentamos ajustar o algoritmo Holt-Winters e o modelo ARIMA, para ver o comportamento deles com as séries, e compará-los entre si. A série do SRAG (Brasil e Pernambuco) com dados semanais, ao longo dos anos pode ser vista na Figura (5). Não podemos ver com clareza se as séries possui tendência ou sazonalidade, mais podemos ver com clareza, nas duas séries, uma quebra estrutural. Isso foi visto anteriormente, como indícios de subnotificações do COVID-19. Figura 5A – Série Temporal da SARG Brasil: Figura 5B – Série Temporal da SARG Pernambuco: Os dois gráficos são bem parecidos, o que é normal, já que estamos apenas filtrando a base de dados do Brasil, para um único estado, Pernambuco. Usando a média de casos por semanas ao longo do anos, é possível tentar identificar se a indícios de sazonalidade. Figura 6A – Média de casos por Semanas ao Longo do Anos no Brasil. Figura 6B – Média de casos por Semanas ao Longo do Anos em Pernambuco. Como existe uma quebra estrutural em ambas as séries, visto na Figura (5), é possível ver uma diferença nas medinas ao longo dos anos. Porém, não é possível saber de forma visual se essa diferença é por uma sazonalidade ou pela quebra estrutural. Fazendo a decomposição das séries em componentes não-observáveis, podemos perceber que não existe indícios de sazonalidade multiplicativa. Também é possível ver certa tendência em ambas as série. A decomposição das séries pode ser vista na Figura (7). Figura 7A – Decomposição das Séries em Componentes Não-Observáveis no Brasil. Figura 7B – Decomposição das Séries em Componentes Não-Observáveis em Pernambuco. Fazendo os gráficos da função autocorrelação(ACF) e da função de autocorrelação parcial(PACF) para as séries Brasil e Pernambuco, podemos ver um decrescimento lento no Gráfico da ACF, em ambos gráficos. Dando indícios que a série tem raiz unitária e precisa ser diferenciada. Também temos indícios que a série não contém sazonalidade aleatória já que não temos alternâncias de picos no gráfico ACF. Figura 8 – Função de Autocorrelação e Função de Autocorrelação Parcial para a Série SRAG Brasil. Figura 9 – Função de Autocorrelação e Função de Autocorrelação Parcial para a Série SRAG Pernambuco. Com os indícios de raiz unitária, foi feito um teste ADF[2], onde obtemos o valor da estatística de teste Valor da estatística de teste para a série Brasil: $-1.5546$ Valor da estatística de teste para a série Pernambuco: $-3.0419$ onde a região critica é dada pela Tabela (3) Tabela 3 – Valores Críticos para o Teste ADF. 1% 5% 10% -2,58 -1,95 -1,62 Como os dois valores da estatística de teste deram menor que o valor tabelado de $1%$, temos que as duas séries precisam ser diferenciada ao nível de $1%$. A partir de agora, ajustaremos os métodos usuais propostos, faremos previsões, e em seguida usaremos algumas medidas de erros para verificar qual dos dois modelos obteve melhor estimativa. ","date":"01-03-2021","objectID":"/dez-post/:3:2","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Algoritmo Hotz-Winters O primeiro método de previsão a ser usado foi a classe de algoritmos de alisamento exponencial, o algoritmo de Holt-Winters. As estimativas dos ajustes para as duas séries podem ser vistas na Tabela (4). Tabela 4A – Valores Estimados Para os Parâmetros de Suavização no Brasil. $\\alpha$ $\\beta$ $\\gamma$ 1 0.2692 0 Tabela 4B – Valores Estimados Para os Parâmetros de Suavização em Pernambuco. $\\alpha$ $\\beta$ $\\gamma$ 1 0 1 Após as estimativas geradas a partir do algoritmo Holt-Winters, foram feitas previsões para 6 semanas em ambas as séries (Brasil e Pernambuco). Podemos ver as previsões pelas Tabelas (5) e (6). Com ela, podemos comparar o valor previsto com o valor verdadeiro. Tabela 5 – Previsões para as próximas 6 semanas para a série SRAG Brasil. Data (Ano/Semana) Valor Real Previsão LI 95% LS 95% 2020/50 10568 10034.03 9164.793 10903.26 2020/51 9484 10486.67 9082.166 11891.17 2020/52 8670 10951.84 9012.596 12891.08 2021/1 9148 11438.39 8942.565 13934.22 2021/2 10223 11880.07 8799.878 14960.26 2021/3 9759 12351.32 8657.342 16045.29 Tabela 6 – Previsões para as próximas 6 semanas para a série SRAG Pernambuco. Data (Ano/Semana) Valor Real Previsão LI 95% LS 95% 2020/50 391 327.0263 260.9630 393.0897 2020/51 400 325.0623 231.6346 418.4899 2020/52 322 325.1078 210.6827 439.5329 2021/1 281 326.1437 194.0170 458.2705 2021/2 283 325.1604 177.4383 472.8826 2021/3 265 325.1771 163.3556 486.9987 Uma observação importante, é que esses intervalos de confiança gerados não são bem definidos por serem construídos a partir da distribuição normal. Para facilitar a visualização das Tabelas (5) e (6), foi construído dois gráficos afim de representar as mesmas. Podemos notar pela Figura (10) que apenas um valor ficou fora do intervalo de confiança. Já na Figura (11), todos os intervalos continham o valor verdadeiro. Figura 10 – Representação da Tabela (5) Figura 11 – Representação da Tabela (6) ","date":"01-03-2021","objectID":"/dez-post/:3:3","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Modelo ARIMA Foram ajustados vários modelos para as séries SRAG, Brasil e Pernambuco, e os modelos escolhidos foram os que continham menor AICc[3], e que passaram na análise de diagnóstico. Para a série SRAG Brasil, foi escolhido o modelo ARIMA(9,1,2). Como vimos anteriormente, a série continha raiz unitária e precisava ser diferenciada. As estimativas e erro padrão podem ser visto na Tabela (7). Tabela 7 – Valores Estimados para o Modelo ARIMA(9,1,2) para a série SRAG Brasil. AR1 AR2 AR3 AR4 AR5 AR6 AR7 AR8 AR9 MA1 MA2 0.547 -0.810 0.172 0.202 0.156 -0.075 0.176 -0.087 -0.099 -0.305 0.940 e.p 0.055 0.072 0.062 0.063 0.063 0.063 0.063 0.053 0.053 0.034 0.050 Ao fazer a análise de diagnóstico, podemos perceber pela Figura (12) que é possível ver alguns ruídos no gráfico dos erros padrões. Esses ruídos são no ano de 2016 e 2020, como falado na análise descritiva, esses anos tiveram uma epidemia e uma pandemia respectivamente. No gráfico da ACF dos resíduos, podemos perceber que nem todos os lags estão dentro do intervalo. Porém, pelo gráfico da estatística de Ljung-Box, não temos nenhum lag significativo, mostrando assim que o modelo escolhido está ajustado. Figura 12 – Diagnostico do Modelo ARIMA(9,1,2) para a série SRAG Brasil Depois do ajuste e da análise de diagnostico do modelo, partiremos para as previsões das próximas seis semanas e compararemos com o valor verdadeiro da série. A Tabela (8) mostra as previsões, valor real e intervalos de confiança. Tabela 8 – Previsões para as próximas 6 semanas para a série SRAG Brasil. Data (Ano/Semana) Valor Real Previsão LI 95% LS 95% 2020/50 10568 9764.18 9011.91 10516.44 2020/51 9484 10043.05 8843.67 11242.42 2020/52 8670 10349.78 8701.34 11998.22 2021/1 9148 10767.93 8717.09 12818.78 2021/2 10223 10713.95 8289.58 13138.32 2021/3 9759 10535.22 7711.05 13359.39 Novamente, uma observação importante é que esses intervalos de confiança gerados não são bem definidos por serem construídos a partir da distribuição normal. Para mais detalhes ver: . Para facilitar o entendimento da Tabela (8), foi construído um gráfico, Figura (13), para visualizar a distancia do valor real para o estimado. Podemos perceber pela Figura (13), que apenas dois intervalos de confiança não continham o valor verdadeiro. Figura 13 – Representação da Tabela (8) Agora, fazendo o ajuste para a série SRAG Pernambuco, foram ajustados vários modelos, e foi escolhido o que continha menor AICc, e que passou na análise de diagnóstico. O modelo escolhido foi um ARIMA(5,1,10). Como vimos anteriormente, a série continha raiz unitária e precisava ser diferenciada. As estimativas e erro padrão podem ser visto na Tabela (9). **Tabela 9 – Valores Estimados para o Modelo ARIMA(5,1,10) para a série SRAG Pernambuco. AR1 AR2 AR3 AR4 AR5 MA1 MA2 MA3 MA4 MA5 MA6 MA7 MA8 MA9 MA10 -0.056 -0.199 0.223 -0.037 0.534 0.275 0.627 0.088 0.361 -0.549 -0.307 -0.474 -0.173 -0.479 -0.114 s.e. 0.163 0.118 0.100 0.087 0.115 0.164 0.107 0.092 0.087 0.097 0.063 0.064 0.066 0.060 0.074 Ao fazer a análise de diagnóstico, podemos perceber pela Figura (14) que é possível ver alguns ruídos no gráfico dos erros padrões. Esses ruídos são mais notórios no ano de 2020, dando mais indícios de subnotificação do COVID-19. No gráfico da ACF dos resíduos, não percebemos nenhum lags fora dentro do intervalo. E ainda, pelo gráfico da estatística de Ljung-Box, não temos nenhum lag significativo, mostrando assim que o modelo escolhido está ajustado. Figura 14 – Diagnostico do Modelo ARIMA(5,1,10) para a série SRAG Pernambuco. Depois do ajuste e da análise de diagnostico do modelo, partiremos para as previsões das próximas seis semanas e compararemos com o valor verdadeiro da série. A Tabela (10) mostra as previsões, valor real e intervalos de confiança. Tabela 10 – Previsões para as próximas 6 semanas para a série SRAG Pernambuco. Data (Ano/Semana) Valor Real Previsão LI 95% LS 95% 2020/50 391 324.78 276.18 373.38 2020/51 400 303.45 226.83 380.07 2020/52 322 293.39 183.","date":"01-03-2021","objectID":"/dez-post/:3:4","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Comparação Holt-Winters x ARIMA A partir de algumas medidas de erros, iremos comparar os dois métodos de previsões e definir qual se adequar melhor os dados. As Tabelas (11) e (12) mostram algumas medidas de erro, e assim como foi mencionando anteriormente, os intervalos de confiança gerados não são bem definidos, por serem construídos a partir da distribuição normal. Com isso, as medidas de erro servem para da uma comparação e precisão dos ajustes. Podemos perceber que tanto o RMSE e MAE do modelo ARIMA é menor que o do algoritmo de Holt-Winters, mostrando assim, que o modelo ARIMA é superior (nas séries analisadas) que o algoritmo de Holt-Winters, tanto no dados de treinamento como no de previsão. Tabela 11A – Medidas de erro, SRAG Brasil (ARIMA). RMSE MAE Treinamento 379.33 132.31 Previsão 1099.09 988.29 Tabela 11B – Medidas de erro, SRAG Brasil (Holt-Winters). RMSE MAE Treinamento 443.03 163.75 Previsão 1880.12 1726.38 Tabela 12A – Medidas de erro, SRAG Pernambuco (ARIMA). RMSE MAE Treinamento 24.41 11.16 Previsão 50.04 37.09 Tabela 12B – Medidas de erro, SRAG Pernambuco (Holt-Winters). RMSE MAE Treinamento 33.68 12.79 Previsão 53.47 48.25 ","date":"01-03-2021","objectID":"/dez-post/:3:5","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Conclusão Como virmos desde a análise descritiva, temos grandes indícios que existe sim subnotificações do COVID-19. Com o ajuste dos modelos, fizemos previsões das próximas seis semanas e o comparamos com o valor real. Vimos que o modelo ARIMA tem melhor ajuste, já que o mesmo contém menor erro. ","date":"01-03-2021","objectID":"/dez-post/:4:0","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Código R library(tidyverse) theme_set(theme_bw() + theme(axis.title = element_text(size =20), axis.text = element_text(size = 16), legend.text=element_text(size=14))) library(reshape2) require(readxl) require(dplyr) require(ggplot2) require(ggfortify) require(tseries) require(urca) require(forecast) require(gridExtra) library(zoo) library(xtable) dados\u003c-read.csv(\"Downloads/Dados_InfoGripe_serie_temporal_com_estimativas_recentes.csv\", sep = \";\", dec = \",\") dados\u003c-dados[,2:10] names(dados)\u003c-c(\"UF\", \"Unidade.da.Federação\", \"Tipo\", \"dado\", \"escala\", \"ano\", \"epiweek\",\"Situação.do.dado\", \"casos\") #### GRAFICO srag_filtrado_brasil \u003c- dados %\u003e% filter(Unidade.da.Federação==\"Brasil\") %\u003e% filter(dado==\"srag\") %\u003e% filter(escala==\"casos\") %\u003e% filter(ano \u003e= 2011) %\u003e% filter(ano \u003c= 2020) srag_filtrado_brasil$ano\u003c-as.factor(srag_filtrado_brasil$ano) srag_filtrado_PE \u003c- dados %\u003e% filter(Unidade.da.Federação==\"Pernambuco\") %\u003e% filter(dado==\"srag\") %\u003e% filter(escala==\"casos\") %\u003e% filter(ano \u003e= 2011) %\u003e% filter(ano \u003c= 2020) srag_filtrado_PE$ano\u003c-as.factor(srag_filtrado_PE$ano) ggplot(srag_filtrado_brasil, aes(x = epiweek, y = casos, group = ano, colour = ano)) + geom_line() + scale_colour_viridis_d() + labs(x = \"Semana\", y = \"Número de Casos\", colour = \"Ano\", title=\"Número de Casos de Síndrome Respiratória Aguda Grave por Ano no Brasil\") ggplot(srag_filtrado_PE, aes(x = epiweek, y = casos, group = ano, colour = ano)) + geom_line() + scale_colour_viridis_d() + labs(x = \"Semana\", y = \"Número de Casos\", colour = \"Ano\", title=\"Número de Casos de Síndrome Respiratória Aguda Grave por Ano em Pernambuco\") srag_medio_BR \u003c- srag_filtrado_brasil %\u003e% filter(ano != \"2020\") %\u003e% group_by(epiweek) %\u003e% summarise(media = mean(casos)) full_join(srag_medio_BR, filter(srag_filtrado_brasil, ano == \"2020\")) %\u003e% select(epiweek, media, casos) %\u003e% melt(id.var=\"epiweek\") %\u003e% mutate(variable = ifelse(variable == \"media\", \"Média: 2011-2019\", \"2020\")) %\u003e% ggplot(., aes(x = epiweek, y = value, group = variable, colour = variable)) + geom_line() + scale_colour_viridis_d() + labs(x = \"Semana\", y = \"Número de Casos\", colour = \"Grupo\", title=\"Número de Casos de Síndrome Respiratória Aguda Grave no Brasil\") srag_medio_PE \u003c- srag_filtrado_PE %\u003e% filter(ano != \"2020\") %\u003e% group_by(epiweek) %\u003e% summarise(media = mean(casos)) full_join(srag_medio_PE, filter(srag_filtrado_PE, ano == \"2020\")) %\u003e% select(epiweek, media, casos) %\u003e% melt(id.var=\"epiweek\") %\u003e% mutate(variable = ifelse(variable == \"media\", \"Média: 2011-2019\", \"2020\")) %\u003e% ggplot(., aes(x = epiweek, y = value, group = variable, colour = variable)) + geom_line() + scale_colour_viridis_d() + labs(x = \"Semana\", y = \"Número de Casos\", colour = \"Grupo\", title=\"Número de Casos de Síndrome Respiratória Aguda Grave em Pernambuco\") #### SERIE TEMPORAL srag_filtrado_brasil_completa \u003c- dados %\u003e% filter(Unidade.da.Federação==\"Brasil\") %\u003e% filter(dado==\"srag\") %\u003e% filter(escala==\"casos\") %\u003e% filter(ano \u003e= 2011) srag_filtrado_brasil_completa\u003c-srag_filtrado_brasil_completa[1:524,] srag_filtrado_PE_completa \u003c- dados %\u003e% filter(Unidade.da.Federação==\"Pernambuco\") %\u003e% filter(dado==\"srag\") %\u003e% filter(escala==\"casos\") %\u003e% filter(ano \u003e= 2011) srag_filtrado_PE_completa\u003c-srag_filtrado_PE_completa[1:524,] serie_brasil_completa\u003c-ts(srag_filtrado_brasil_completa$casos, start = c(2011,1),frequency = 52) serie_brasil_treino\u003c-window(serie_brasil_completa, start = c(2011,1),end=c(2020,49)) serie_brasil_teste\u003c-window(serie_brasil_completa, start=c(2020,50)) serie_PE_completa\u003c-ts(srag_filtrado_PE_completa$casos, start = c(2011,1),frequency = 52) serie_PE_treino\u003c-window(serie_PE_completa,start = c(2011,1), end=c(2020,49)) serie_PE_teste\u003c-window(serie_PE_completa,start=c(2020,50)) autoplot(serie_brasil_completa, xlab = \"Número de Casos\", ylab = \"Tempo\", main = \"Série Temporal do Número de Casos de Síndrome Respiratória Aguda Grave no Brasil\") + geom_point(size=0.5) + scale_colour_viridis_d() autoplot(serie_PE_completa,xlab = \"Número de Casos\", ylab = \"Tempo\"","date":"01-03-2021","objectID":"/dez-post/:4:1","tags":null,"title":"Previsões e possíveis casos de subnotificação do COVID-19 na síndrome respiratória aguda grave (SRAG).","uri":"/dez-post/"},{"categories":null,"content":"Esse trabalho foi apresentado na disciplina de Estatística computacional ministrada pelo Prof. Dr. Francisco Cribari Neto. A distribuição gama é útil na modelagem de variáveis aleatórias contínuas que são maiores que zero. Ela é comumente usada em estudos de sobrevivência. A distribuição gama também é útil na modelagem de dados com alta variabilidade. Estimar apropriadamente os parâmetros da distribuição gama é vital para o estudo desse e de outras distribuições. Dependendo dos valores de seus parâmetros, a distribuição gama assume várias formas, incluindo curvas em sino semelhante à distribuição normal. Este trabalho contém um estudo de simulação sobre os estimadores de máxima verossimilhança, usando o método iterativo BFGS para estimação dos parâmetros da distribuição gama. A simulação será conduzida para determinar estimativas, intervalos de confiança, viés relativo e tempo de execução do algoritmo nas linguagens de programação R e Ox. ","date":"02-09-2020","objectID":"/quinto-post/:0:0","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Revisão da Literatura Nesta seção será apresentada a teoria básica sobre simulações de Monte Carlo, método de máxima verossimilhança e método iterativo BFGS. Após isso, foi usado a distribuição gama para mostrar cada método passo a passo. ","date":"02-09-2020","objectID":"/quinto-post/:1:0","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Monte Carlo Suponha que se quer estimar $\\theta$, o valor esperado de alguma variável aleatória $X$: $$\\begin{aligned} \\theta = E(X).\\end{aligned}$$ Suponha, além disso, que possam ser gerados valores de variáveis aleatórias independentes com a mesma distribuição de probabilidade de $X$. Cada vez que for gerado um novo valor, diz-se que uma simulação foi concluída. Suponha que vão ser realizadas $n$ simulações, assim, serão gerados $X_1,X_2,\\dots,X_n$. Se $$\\begin{aligned} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i,\\end{aligned}$$ for sua média, então pela lei forte dos grandes números, $\\bar{X}$ será usado como um estimador para $\\theta$. Para o valor esperado e variância tem-se $$\\begin{aligned} E(\\bar{X}) = \\frac{1}{n} \\sum_{i=1}^{n} E(X_i) = \\theta,\\end{aligned}$$ e $$\\begin{aligned} Var(\\bar{X}) = \\frac{\\sigma^2}{n}.\\end{aligned}$$ Além disso, decorre do Teorema Central do Limite que, para $n$ grande, $\\bar{X}$ terá uma distribuição normal aproximada. Assim, se $\\sigma/\\sqrt{n}$ é pequeno, então $\\bar{X}$ tende a estar próximo de $\\theta$ e, quando $n$ for grande, $\\bar{X}$ será um bom estimador para $\\theta$. Esta abordagem para estimar um valor esperado é conhecida como a simulação de Monte Carlo. ","date":"02-09-2020","objectID":"/quinto-post/:1:1","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"O Método de Máxima Verossimilhança Segundo, os estimadores de Máxima Verossimilhança desfrutam de propriedades desejáveis e podem ser usados na construção de intervalos de confiança e testes de hipóteses. A ideia da estimação por máxima verossimilhança é de encontrar os parâmetros que maximizam a probabilidade de que um certo conjunto de dados sejam gerados por algum processo específico. Agora, imagine que temos uma amostra aleatória, que não sabemos qual distribuição os dados seguem, mas vamos supor que eles seguem alguma distribuição específica, por exemplo, uma Gama. O formato da Gama depende dos parâmetros $(\\alpha,\\beta)$, então, a ideia é de encontrar os valores de $(\\alpha,\\beta)$ que melhor se aproximam da distribuição empírica/observada dos dados. Formalmente, suponhamos $x_1,x_2,…,x_n$ uma amostra aleatória de tamanho $n$ caracterizada pela função de probabilidade ou densidade $f(x)$, a função de verossimilhança para o parâmetro $\\theta$ é dada por: $$\\begin{align} L(\\theta|x_i) = \\prod_{i=1}^{n} f(x_i|\\theta).\\end{align}$$ Então para determinar a melhor distribuição para a amostra, é preciso determinar o valor de $\\theta$ que maximiza $L(\\theta)$. Porém, maximizar a $L(\\theta)$ muitas vezes não é uma tarefa fácil. Dessa forma, foi criado uma método chamado de log-verossimilhança, onde maximizar o $l(\\theta|x_i)=\\log(L(\\theta|x_i))$ é o mesmo que maximizar o $L(\\theta|x_i)$. Propriedades dos Estimadores de Máxima Verossimilhança A teoria dos estimadores de máxima verossimilhança nos garante que eles são consistentes (i.e. que eles aproximam o valor verdadeiro dos parâmetros) e normalmente assintóticos (a distribuição assintótica segue uma distribuição normal) desde que algumas condições de regularidade sejam atendidas. A consistência dos estimadores $\\hat{\\theta}_{MV}$ significa que eles se aproximam dos valores verdadeiros do parâmetros $\\theta_0$ à medida que aumenta o tamanho da amostra. Isto é, se tivermos uma amostra grande então podemos ter confiança de que nossos estimadores estão muito próximos dos valores verdadeiros dos parâmetros. \\begin{align} \\hat{\\theta}_{MV} \\rightarrow \\theta_0 \\end{align} Dizemos que os estimadores de máxima verossimilhança são normalmente assintóticos porque a sua distribuição assintótica segue uma normal padrão. Portanto, ao utilizarmos o método de Máxima Verossimilhança para estimar os parâmetros de alguma distribuição, na verdade estamos calculando estimadores pontuais. Por sua vez, também e possível obter intervalos de confiança para os parâmetros estimados. Para tanto, faz-se necessário o uso de propriedades adequadas para grandes amostras dos estimadores. Frente a isso, e considerando a normalidade assintótica dos estimadores de Máxima verossimilhança, obtém-se os intervalos de confiança para os parâmetros da distribuição, ou seja: \\begin{align} IC(\\theta,(1-\\alpha)) = \\left( \\hat{\\theta} - z_{\\alpha/2}\\sqrt{Var(\\hat{\\theta})} ; \\hat{\\theta} + z_{\\alpha/2}\\sqrt{Var(\\hat{\\theta})}\\right)\\end{align} em que $(1-\\alpha)%$ representa a porcentagem de confiança e $z_{\\alpha/2}$ representa o quantil da distribuição normal. ","date":"02-09-2020","objectID":"/quinto-post/:1:2","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Distribuição Gama Se $X$ é uma variável aleatória com distribuição Gama e parâmetros $\\alpha \u003e 0$ e $\\beta \u003e 0$, denotando-se $X \\sim \\ \\text{Gama}(\\alpha,\\beta)$, sua função densidade é dada por $$f(x)= \\dfrac{x^{\\alpha - 1}e^{-\\beta x}\\beta^\\alpha}{\\Gamma(\\alpha)} \\hspace{0.5cm} \\text{quando} \\hspace{0.5cm} x \\geq 0$$ A Figura abaixo, representa a distribuição gama com alguns parâmetros. A mesma mostra que os parâmetros $\\alpha$ e $\\beta$ servem como parâmetros de escala e forma respectivamente. A variável Gama foi criada como uma extensão da exponencial para descrever o tempo de espera até que um certo número de eventos ocorram, dada uma taxa constante de ocorrência. Devido à sua flexibilidade, posteriormente foi adotada como modelo heurístico para descrever variáveis com distribuições de probabilidade assimétricas. Verossimilhança e log-Verossimilhança para a Distribuição Gama A função de verossimilhança para a distribuição gama é dada por: $$L(\\alpha,\\beta|x_i)=\\prod^n_{i=1}f(x_i;(\\alpha,\\beta))=\\prod^n_{i=1}\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}_i e^{-\\beta x_i}$$ Com isso, a função log-verossimilhança é dada por: $$\\begin{aligned} l(\\theta|x_i)=\\log L(\\alpha,\\beta)=n\\alpha\\log\\beta -n\\log \\Gamma(\\alpha)-\\beta \\sum^n_{i=1}x_i+(\\alpha-1)\\sum^n_{i=1}\\log x_i\\end{aligned}$$ Para distribuição Gama a função escore é dada por: $$\\begin{aligned} U(\\theta)=\\dfrac{\\partial \\log L(\\theta)}{\\partial \\theta}=\\left(\\dfrac{\\partial \\log L(\\alpha,\\beta)}{\\partial \\alpha}, \\dfrac{\\partial \\log L(\\alpha,\\beta)}{\\partial \\beta}\\right)^\\top, \\end{aligned}$$ em que $$\\begin{aligned} \\dfrac{\\partial \\log L(\\alpha,\\beta)}{\\partial \\alpha}\u0026= n\\log \\beta-n\\frac{\\Gamma^\\prime (\\alpha)}{\\Gamma (\\alpha)}+\\sum^n_{i=1}\\log x_i,\\ \\dfrac{\\partial \\log L(\\alpha,\\beta)}{\\partial \\beta}\u0026= \\frac{n\\alpha}{\\beta}-\\sum^n_{i=1}x_i.\\end{aligned}$$ Com isso, tomamos $\\widehat{U}(\\theta)=0$, ou seja, $\\frac{\\partial \\log L(\\alpha,\\beta)}{\\partial \\alpha} =0$ e $\\frac{\\partial \\log L(\\alpha,\\beta)}{\\partial \\beta}=0$. Obtemos os estimadores para $\\alpha$ e $\\beta$: $$\\hat{\\alpha}=\\bar{x}\\hat{\\beta}$$ $$\\hat{\\beta}= \\exp \\left( \\dfrac{\\Gamma’(\\alpha)}{\\Gamma(\\alpha)} - \\sum_{i=1}^n \\dfrac{\\log x_i}{n} \\right)$$ O sistema de equações precedente não admite forma fechada, sendo necessário a utilização de métodos numéricos para resolve-lo. Na literatura existem diversos algoritmos para maximização que envolvem uma estimativa inicial $\\theta_0$ para $\\theta$ e um processo iterativo que constrói uma sequencia de estimadores que convergem para $\\theta$. Em nosso caso em particular, computaremos as estimativas dos parâmetros pela maximização numérica por meio do algoritmo de otimização não linear quasi-Newton, conhecido como BFGS (Broyden-Fletcher-Goldfarb-Shanno). O algoritmo BFGS é similar ao método de Newton-Raphson, distinguindo-se apenas pelo fato de utilizar uma sequencia de matrizes simétricas e positivas definidas $B^k$ em vez da oposta da hessiana $-H^{-1}$, de tal forma que $$\\begin{aligned} \\lim_{k\\rightarrow \\infty} B^k = -H^{-1}.\\end{aligned}$$ Normalmente, utiliza-se a matriz identidade de mesma ordem como matriz inicial $B^0$, isso porque uma matriz identidade é sempre positiva definida e simétrica, propagando-se para aproximações $B^k$ positivas definidas e simétricas. ","date":"02-09-2020","objectID":"/quinto-post/:1:3","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Metodologia Neste trabalho foram consideradas algumas estratégias para avaliar os estimadores de máxima verossimilhança com o método iterativo BFGS. Foram realizadas 10.000 replicas de Monte Carlo para esse estudo, e em cada replica, foi verificado a estimativa dos parâmetros, erro padrão, intervalos de confiança e viés. Todo o algoritmo foi feito nas linguagens de programação R e Ox e as mesmas se encontram no apêndice. Das informações relacionadas as versões dos softwares para as simulações são: versão 4.0.2, utilizando a função optim da biblioteca stats. A versão do Ox, , é a 8.02, e foi utilizada a função MaxBFGS da biblioteca maximize. A respeito dos geradores de números aleatórios, optou-se por considerar os geradores defaults de ambas linguagens. Sendo rgamma() para o R e rangamma para o Ox. ","date":"02-09-2020","objectID":"/quinto-post/:2:0","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Estudo de Simulação Nessa seção serão apresentados os resultados das simulações para obtenção das estimativas obtidas pelo método iterativo BFGS com o estimador de máxima verossimilhança. Foram gerados amostras de tamanho $n=(10,20,30,50,100,200,300,500,1000)$ da distribuição gama variando os parametros de escala e forma. Na Tabela abaixo estão apresentados os resultados para estimativas do parâmetro de escala $\\alpha$. Para este cenário fixamos o parâmetro de forma em $\\beta = 1$, e variamos o valor de $\\alpha$ em 1 e 2, com isso realizamos $10.000$ replicas de Monte Carlo. Note pela Tabela que quando aumentamos o tamanho da amostra o viés relativo decresce rapidamente para próximo de zero, isso em ambas linguagens de programação. Podemos ainda considerar que para amostras maiores que $20$ o método de iteração BFGS obteve boa aproximação para o valor verdadeiro. Foi construindo um intervalo de confiança com $95%$ de confiança sobre a distribuição normal, pois virmos que quando o $n$ tende a um valor muito grande os estimadores de máxima verossimilhança se aproximam da distribuição normal, veremos isso com mais clareza na próxima figura. Simulação no R n $\\alpha$ $\\hat{\\alpha}$ IC$(\\hat{\\alpha})$ Viés Relativo $(\\hat{\\alpha})$ n=10 1 1.3461 [0.2755 ; 2.4167] 34.61% 2 2.7649 [0.4687 ; 5.0612] 38.25% n=20 1 1.1410 [0.5095 ; 1.7725] 14.10% 2 2.3180 [0.9705 ; 3.6655] 15.90% n=30 1 1.0889 [0.5992 ; 1.5787] 8.89% 2 2.2028 [1.1606 ; 3.2449] 10.14% n=50 1 1.0535 [0.6878 ; 1.4191] 5.35% 2 2.1153 [1.3422 ; 2.8883] 5.76% n=100 1 1.0277 [0.7762 ; 1.2793] 2.77% 2 2.0523 [1.5230 ; 2.5816] 2.62% n=200 1 1.0128 [0.8378 ; 1.1879] 1.28% 2 2.0272 [1.6578 ; 2.3965] 1.35% n=300 1 1.0089 [0.8666 ; 1.1512] 0.89% 2 2.0172 [1.7172 ; 2.3171] 0.86% n=500 1 1.0047 [0.8950 ; 1.1144] 0.47% 2 2.0122 [1.7805 ; 2.2440] 0.61% n=1000 1 1.0027 [0.9253 ; 1.0801] 0.27% 2 2.0060 [1.8427 ; 2.1693] 0.30% As figuras abaixo representa a tabela acima. Simulação no Ox n $\\alpha$ $\\hat{\\alpha}$ IC$(\\hat{\\alpha})$ Viés Relativo $(\\hat{\\alpha})$ n=10 1 1.4099 [-0.5363 ; 3.3561] 40.99% 2 2.7733 [0.9286 ; 4.6181] 38.67% n=20 1 1.1762 [0.7726 ; 1.5797] 17.62% 2 2.3250 [1.8151 ; 2.8349] 16.25% n=30 1 1.1157 [0.6223 ; 1.6092] 11.57% 2 2.1980 [1.2346 ; 3.1614] 9.90% n=50 1 1.0720 [0.5982 ; 1.5457] 7.20% 2 2.1166 [1.5333 ; 2.7000] 5.83% n=100 1 1.0397 [0.8051 ; 1.2744] 3.97% 2 2.0579 [1.4068 ; 2.7090] 2.89% n=200 1 1.0237 [0.8582 ; 1.1892] 2.37% 2 2.0299 [1.6824 ; 2.3774] 1.50% n=300 1 1.0174 [0.8740 ; 1.1609] 1.74% 2 2.0209 [1.6906 ; 2.3512] 1.05% n=500 1 1.0127 [0.8950 ; 1.1303] 1.27% 2 2.0126 [1.7935 ; 2.2317] 0.63% n=1000 1 1.0082 [0.9336 ; 1.0829] 0.82% 2 2.0078 [1.8484 ; 2.1673] 0.39% As figuras abaixo representa a tabela acima. Podemos ver que a medida que o tamanho da amostra aumenta as estimativas se aproximam do valor verdadeiro (linha vermelha). Além disso, todos os intervalos de confiança contém o valor do parâmetro verdadeiro. Como falado anteriormente, as estimativas dos parâmetros tem distribuição assintoticamente normal. Pela Figura abaixo, podemos ver com clareza que a estimativa do parâmetro $\\hat{\\alpha}$, a medida que a amostra aumenta tem distribuição assintoticamente normal. As mesmas estimativas foram feitas para o parâmetro $\\beta$ ","date":"02-09-2020","objectID":"/quinto-post/:3:0","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Tempo de Execução Nota-se pela Tabela abaixo que o tempo de execução na linguagem de programação em R é superior a do Ox, tornando a assim uma linguagem menos atraente para simulações mais pesadas. Vale ressaltar que o Ox é aproximadamente 2.1x mais rápido que o R. Tamanho R Ox 10 29.63 4.40 20 29.52 4.75 30 28.81 4.96 50 29.07 5.50 100 29.65 7.02 200 32.71 10.37 300 35.12 14.75 500 42.55 20.90 1000 60.20 38.97 A figura abaixo representa a tabela acima. ","date":"02-09-2020","objectID":"/quinto-post/:3:1","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Conclusão O trabalho mostrou toda a parte teórica sobre estimadores de máxima verossimilhança através de simulações. Simulada replicas de Monte Carlo, vimos que os estimadores de máxima verossimilhança através do método BFGS são bastante eficientes para a estimativa do parâmetro, com amostras grandes os valores estimados chegam muito próximo do valor verdadeiro. Observamos também na prática, que os parâmetros estimados seguem assintoticamente a distribuição normal. Em relação as linguagens de programação R e Ox, podemos dizer que o a programação em R é mais eficiente em encontrar o valor verdadeiro, tendo que o viés relativo sempre ficou menor em comparação com o Ox. Já o Ox é mais eficiente em relação ao tempo de execução, tornando a linguagem mais atraente para simulações mais pesadas. ","date":"02-09-2020","objectID":"/quinto-post/:4:0","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"R # Maximizando a distribuição gama em R # Computacional-Mestrado (UFPE) # Jerfson Bruno library(ggplot2) library(xtable) library(gridExtra) max_gama\u003c-function(a,b,n){ dados \u003c- data.frame(amostra = rgamma(n, a, b)) n_obs \u003c- nrow(dados) llf\u003c- function(x){ a \u003c- x[1] b \u003c- x[2] lgama\u003c-((a-1)*sum(log(dados$amostra))-b*sum(dados$amostra)-n_obs*a*log(1/b)-n_obs*log(gamma(a))) return(-lgama) } q \u003c- optim(c(1.0, 1.0), llf, hessian = TRUE) return(q) } set.seed(89) rm\u003c-10000 a=1 b=1 n\u003c-c(10,20,30,50,100,200,300,500,1000) estimativas_n\u003c-data.frame(n=n,alpha=rep(NA,length(n)),beta=rep(NA,length(n))) for (i in 1:length(n)){ contador=1 estimativas\u003c-data.frame(alpha=rep(NA,rm),beta=rep(NA,rm)) while(contador\u003c=rm){ z\u003c-max_gama(a,b,as.numeric(n[2])) if(z$convergence==0){ estimativas$alpha[contador]\u003c-z$par[1] estimativas$beta[contador]\u003c-z$par[2] estimativas$dp_alpha[contador]\u003c-sqrt(diag(solve(z$hessian)))[1] estimativas$dp_beta[contador]\u003c-sqrt(diag(solve(z$hessian)))[2] contador=contador+1 write.csv(estimativas,paste('11-',paste(as.character(n[i]),'.txt',sep = '')), row.names = FALSE) } } estimativas_n$alpha[i]\u003c-mean(estimativas$alpha) estimativas_n$beta[i]\u003c-mean(estimativas$beta) estimativas_n$ep_alpha[i]\u003c-mean(estimativas$dp_alpha) estimativas_n$ep_beta[i]\u003c-mean(estimativas$dp_beta) estimativas_n$aIC_I[i]\u003c-mean(estimativas$alpha)-1.96*mean(estimativas$dp_alpha) estimativas_n$aIC_S[i]\u003c-mean(estimativas$alpha)+1.96*mean(estimativas$dp_alpha) estimativas_n$bIC_I[i]\u003c-mean(estimativas$beta)-1.96*mean(estimativas$dp_beta) estimativas_n$bIC_S[i]\u003c-mean(estimativas$beta)+1.96*mean(estimativas$dp_beta) estimativas_n$Vies_a[i]\u003c-(mean(estimativas$alpha) - a)/a*100 estimativas_n$Vies_b[i]\u003c-(mean(estimativas$beta) - b)/b*100 } write.csv(estimativas_n, 'gama11_R.txt', row.names = FALSE) ","date":"02-09-2020","objectID":"/quinto-post/:5:0","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Ox ###Programação em Ox // Maximizando a distribuição gama em Ox // Computacional-Mestrado (UFPE) // Jerfson Bruno #include\u003coxstd.h\u003e #import\u003cmaximize\u003e // #include\u003coxprob.h\u003e decl n = 1000; decl Rmc = 10000; static decl vetor_gamma; gamma(const vetor_parametros, const adFunc, const escore, const hessiana){ decl t, cont; decl a1 = vetor_parametros[0]; decl a2 = vetor_parametros[1]; decl vone = ones(1, n); adFunc[0] = (a1-1)*(vone*(log(vetor_gamma)))-a2*(vone*(vetor_gamma)) -n*a1*log(1/a2)-n*log(gammafact(a1)); if(escore){ (escore[0])[0] = n*log(a2) - n*(gammafact(a1)/gammafact(a1)) + (vone*(log(vetor_gamma))); (escore[0])[1] = (n*a1)/a2 -(vone*(vetor_gamma)); } if(isnan(adFunc[0])||isdotinf(adFunc[0])) return 0; else return 1; } main(){ decl maxgama, i, var1, variancia, dp, vp, dfunc, mhess, vies; decl alpha = 2.0, beta = 2.0; decl v = \u003c2.0, 2.0\u003e; decl cont = 0; decl exectime; decl ep; decl v_estimativa = zeros(Rmc, 2); decl media; exectime = timer(); ranseed(\"MWC_32\"); ranseed(89); for(i = 0; i \u003c Rmc; i++){ vetor_gamma = rangamma(n, 1, alpha, beta); vp = \u003c1; 1\u003e; maxgama = MaxBFGS(gamma, \u0026vp, \u0026dfunc, 0, TRUE); if(maxgama == MAX_CONV || maxgama == MAX_WEAK_CONV){ v_estimativa[i][] = vp'; media = meanc(v_estimativa); variancia = varc(v_estimativa); dp = sqrt(variancia); } else{ i--; cont++; } } Num2Derivative(gamma, vp, \u0026mhess); vies = media - v; ep = sqrt(diagonal(invertsym(-mhess))); decl a1 = vies[0]/v'[0], a2 = vies[1]/v'[1]; decl Linf = double(dropc(media,1) - (1.96)*dropc(ep,1)); decl Lsup = double(dropc(media,1) + (1.96)*dropc(ep,1)); decl Linfbeta = double(dropc(media,0) - (1.96)*dropc(ep,0)); decl Lsupbeta = double(dropc(media,0) + (1.96)*dropc(ep,0)); println(\"Convergência: \", MaxConvergenceMsg(maxgama)); println(\"Amostras:\\n \", n); println(\"Número de réplicas de Monte Carlo:\\n \", Rmc); println(\"Número de falhas:\\n \", cont); println(\"Parâmetro verdadeiro de alpha e beta: \\n \", \"[\", alpha, \" ; \", beta,\"]\"); println(\"Valores estimados para alpha e beta:\\n \", \"%10.4f\", media); println(\"Viés relativo para alpha e beta: \\n \",\"[\", a1*100,\" ; \", a2*100,\"]\"); println(\"Intervalo de confiança para alpha:\\n \",\"[\", Linf, \" ; \", Lsup,\"]\"); println(\"Intervalo de confiança para beta:\\n \", \"[\", Linfbeta, \" ; \", Lsupbeta,\"]\"); println(\"Variância das estimativas:\\n \", \"%10.4f\", variancia); println(\"Desvio padrão das estimativas:\\n \", \"%10.4f\", dp); println(\"Erro padrão assintótico:\\n \", \"%10.4f\", ep); println(\"Tempo de execução:\\n \", timespan(exectime)); } ","date":"02-09-2020","objectID":"/quinto-post/:6:0","tags":null,"title":"(Ox vs R): Otimização não linear da distribuição Gamma","uri":"/quinto-post/"},{"categories":null,"content":"Esse trabalho foi apresentado na disciplina de teoria assintotica ministrada pelo Prof. Dr. Gauss Moutinho Cordeiro. O objetivo deste trabalho é usar uma ferramenta matemática conhecida como expansão de Edgeworth em um conjunto de dados gerados a partir da distribuição lomax. Tal expansão permite obter uma função densidade de probabilidade com assimetria e curtose arbitrárias a partir de uma densidade normal. Expansão de Edgeworth: Distribuição Lomax As fórmulas a baixo são as expansões de Edgeworth para as funções densidade e de distribuição de uma soma padronizada $S_n^*$, respectivamente. ","date":"18-03-2020","objectID":"/sexto-post/:0:0","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Função densidade de Edgeworth $$f_{S_n^*} (y) = \\phi (y) \\left\\lbrace 1+ \\frac{\\rho_3}{6\\sqrt{n}} H_3 (y) + \\frac{\\rho_4}{24n} H_4 (y) + \\frac{\\rho_3^2}{72n} H_6 (y)\\right\\rbrace + O(n^{-3/2})$$ ","date":"18-03-2020","objectID":"/sexto-post/:1:0","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Função de distribuição de Edgeworth $$ F_{S_n^*} (y) = \\Phi(y) - \\phi (y) \\left\\lbrace 1+ \\frac{\\rho_3}{6\\sqrt{n}} H_2 (y) + \\frac{\\rho_4}{24n} H_3 (y) + \\frac{\\rho_3^2}{72n} H_5 (y)\\right\\rbrace + O(n^{-3/2})$$ ","date":"18-03-2020","objectID":"/sexto-post/:2:0","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Função densidade da lomax A distribuição Lomax, também chamada de distribuição Pareto Tipo II, é uma distribuição de probabilidade de cauda pesada usada em negócios, economia, ciência atuarial, teoria de filas e modelagem de tráfego da Internet. O criador, Lomax (1987), utilizou a distribuição inicialmente em análises de dados de tempo de vida de falhas de negócios. Uma variável aleatória $X$ segue a distribuição de Lomax com parâmetros $\\lambda \u003e0$ e $\\alpha \u003e0$ se sua função de probabilidade for dada por $$f(x;\\alpha,\\lambda)={\\alpha \\over \\lambda }\\left[{1+{x \\over \\lambda }}\\right]^{-(\\alpha +1)}$$ A densidade pode ser reescrita de tal forma que mostre mais claramente a relação com a distribuição de Pareto Tipo I. Isso é: $$f(x;\\alpha,\\lambda)={{\\alpha \\lambda ^{\\alpha }} \\over {(x+\\lambda )^{\\alpha +1}}}.$$ Os momentos da distribuição Lomax são obtidos pela Equação (4); É possível ver a demonstração analítica em Gradshteyn and Ryzhik (2007). $$\\mathbf{E}(X^r) = \\dfrac{\\alpha \\lambda^r \\Gamma(r+1) \\Gamma(\\alpha-r)}{\\Gamma(\\alpha +1)}, \\hspace{0.5cm} \\alpha\u003er$$ Logo, dado a Equação (5) a média e a variância de $X$, são, respectivamente, $$\\mathbf{E}(x) = \\frac{\\lambda}{\\alpha-1}, \\hspace{0.5cm} \\alpha\u003e1$$ e $$Var(x) = {\\begin{cases}{\\dfrac{\\lambda^{2}\\alpha}{(\\alpha -1)^{2}(\\alpha -2)}} \\hspace{0.5cm} \\alpha \u003e2\\end{cases}}.$$ O coeficiente de assimetria da distribuição Lomax é dado por $$\\rho_3= {\\frac {2(1+\\alpha )}{\\alpha -3}},{\\sqrt {\\frac {\\alpha -2}{\\alpha }}}{\\text{ para }}\\alpha \u003e3,$$ Respeitando a condição de existência, $\\alpha\u003e3$, a distribuição Lomax é sempre assimétrica à direita, independentemente dos valores dos parâmetros. O coeficiente de excesso de curtose da distribuição Lomax é $$\\rho_4= \\frac {6(\\alpha ^{3}+\\alpha ^{2}-6\\alpha -2)}{\\alpha (\\alpha -3)(\\alpha -4)}\\hspace{0.5cm}{\\text{ para }}\\alpha \u003e4$$ Respeitando a condição de existência, $\\alpha\u003e4$ a distribuição Lomax é leptocúrtico, independentemente dos valores dos parâmetros. Além da função densidade há outras formas de se caracterizar as distribuições de probabilidade. Para tanto, pode-se utilizar a função geratriz de momentos, função de cumulantes ou a função de distribuição; vale notar que nem todas as distribuições de probabilidade possuem função geratriz de momentos. A função geratriz de momentos de $X$ é dada por $$M_X (t) = \\alpha (-t \\lambda)^\\alpha e^{(-t\\lambda)} \\Gamma(-\\alpha, -t \\lambda)$$ e a função de cumulantes: $$K_X(t) = \\log(M_Y (t))= \\log (\\alpha ) - \\alpha \\log(t\\lambda) - t \\lambda + \\log(\\Gamma(-\\alpha, -t\\lambda))$$ ","date":"18-03-2020","objectID":"/sexto-post/:3:0","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Função densidade de Edgeworth aplicada a lomax $$f_{S_n^*} (y) = \\phi (y) \\left\\lbrace 1+ \\frac{{\\displaystyle {\\frac {2(1+\\alpha )}{\\alpha -3}},{\\sqrt {\\frac {\\alpha -2}{\\alpha }}}}}{6\\sqrt{n}} H_3 (y) + \\frac{{\\displaystyle {\\frac {6(\\alpha ^{3}+\\alpha ^{2}-6\\alpha -2)}{\\alpha (\\alpha -3)(\\alpha -4)}}}}{24n} H_4 (y) + \\frac{\\left({\\displaystyle {\\frac {2(1+\\alpha )}{\\alpha -3}},{\\sqrt {\\frac {\\alpha -2}{\\alpha }}}} \\right)^2}{72n} H_6 (y)\\right\\rbrace$$ ","date":"18-03-2020","objectID":"/sexto-post/:3:1","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Função distribuição de Edgeworth aplicada a lomax $$F_{S_n^*} (y) = \\Phi(y) - \\phi (y) \\left\\lbrace 1+ \\frac{{\\displaystyle {\\frac {2(1+\\alpha )}{\\alpha -3}},{\\sqrt {\\frac {\\alpha -2}{\\alpha }}}}}{6\\sqrt{n}} H_2 (y) + \\frac{{\\displaystyle {\\frac {6(\\alpha ^{3}+\\alpha ^{2}-6\\alpha -2)}{\\alpha (\\alpha -3)(\\alpha -4)}}}}{24n} H_3 (y) + \\frac{\\left({\\displaystyle {\\frac {2(1+\\alpha )}{\\alpha -3}},{\\sqrt {\\frac {\\alpha -2}{\\alpha }}}} \\right)^2}{72n} H_5 (y)\\right\\rbrace$$ Além disso, a lomax tem varias relações com outras distribuições, algumas delas: Pareto Pareto generalizada prime beta distribuição F distribuição q-exponencial distribuição logística Outras ","date":"18-03-2020","objectID":"/sexto-post/:3:2","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Simulação Sejam $X_1,…,X_n$ variáveis aleatórias com distribuição Lomax, com parâmetros $\\lambda=5$, $\\alpha=5$. As expansões de Edgeworth para a função densidade com tamanhos n=10, n=50, n=100 e n=200, são apresentadas na Figura a seguir. ","date":"18-03-2020","objectID":"/sexto-post/:4:0","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"n=10 ","date":"18-03-2020","objectID":"/sexto-post/:4:1","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"n=50 ","date":"18-03-2020","objectID":"/sexto-post/:4:2","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"n=100 ","date":"18-03-2020","objectID":"/sexto-post/:4:3","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"n=200 ","date":"18-03-2020","objectID":"/sexto-post/:4:4","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"algoritmo para a construção das funções densidade e de distribuição. library(EQL) fp = function(y,n){ (sqrt(n)*(n+y*sqrt(n))^(n-1)*exp(-n-y*sqrt(n)))/factorial(n-1) } edgeworth = function(y, n, rho3, rho4){ dnorm(y)*(1+(rho3*hermite(y,3)/(6*sqrt(n)))+((rho4*hermite(y,4))/(24*n)) +((rho3*rho3*hermite(y,6))/(72*n))) } ","date":"18-03-2020","objectID":"/sexto-post/:5:0","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Distribuição Lomax library(Renext) library(ggplot2) n =100 l = 2 a = 5 mu = l/(a-1) sigma2 = (a*l^2)/((a-1)^2*(a-2)) rho3 \u003c- log((2*(1+a)*sqrt((a-2)/a))/(a-3)) rho4 \u003c- log((6*(a^3+a^2-6*a-2))/(a*(a-3)*(a-4))) x = seq(0.0001, 6, 0.1) y = dlomax(x, scale = l, shape = a) dados\u003c-data.frame(x,y) ggplot(dados, aes(x,y)) + geom_line(color=2) ","date":"18-03-2020","objectID":"/sexto-post/:5:1","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Aproximação Normal pela expansão de Edgeworth para n= 10, 25, 50 e 100. Para n=10 library(EnvStats) n=10 m = 1000 obs = rlomax(m*n, scale = l, shape = a) # Distribuição empírica da soma estocástica de Lomax x = matrix(0, m, n) for(i in 1:m){ x[i,] = rlomax(n, scale = l, shape = a) } sn = apply(x, 1, sum) #Trabalhando com a soma estocástica padronizada empírica snp = (sn-n*mu)/(sqrt(n*sigma2)) x \u003c- qemp(p = seq(0, 1, len = 100), obs = snp) y \u003c- demp(x, snp) z \u003c- dnorm(x) h \u003c- edgeworth(x, n, rho3 = rho3, rho4 = rho4) dados\u003c-data.frame(x,y,z,h) ggplot(dados, aes(x,y)) + geom_path(aes(color = 'Distribuição empírica')) + geom_path(aes(x,z, color = 'Normal'))+ geom_path(aes(x,h, color = 'Expansão Edgeworth')) Para n=25 library(EnvStats) n=25 m = 1000 obs = rlomax(m*n, scale = l, shape = a) # Distribuição empírica da soma estocástica de Lomax x = matrix(0, m, n) for(i in 1:m){ x[i,] = rlomax(n, scale = l, shape = a) } sn = apply(x, 1, sum) #Trabalhando com a soma estocástica padronizada empírica snp = (sn-n*mu)/(sqrt(n*sigma2)) x \u003c- qemp(p = seq(0, 1, len = 100), obs = snp) y \u003c- demp(x, snp) z \u003c- dnorm(x) h \u003c- edgeworth(x, n, rho3 = rho3, rho4 = rho4) dados\u003c-data.frame(x,y,z,h) ggplot(dados, aes(x,y)) + geom_path(aes(color = 'Distribuição empírica')) + geom_path(aes(x,z, color = 'Normal'))+ geom_path(aes(x,h, color = 'Expansão Edgeworth')) Para n=50 library(EnvStats) n=50 m = 1000 obs = rlomax(m*n, scale = l, shape = a) # Distribuição empírica da soma estocástica de Lomax x = matrix(0, m, n) for(i in 1:m){ x[i,] = rlomax(n, scale = l, shape = a) } sn = apply(x, 1, sum) #Trabalhando com a soma estocástica padronizada empírica snp = (sn-n*mu)/(sqrt(n*sigma2)) x \u003c- qemp(p = seq(0, 1, len = 100), obs = snp) y \u003c- demp(x, snp) z \u003c- dnorm(x) h \u003c- edgeworth(x, n, rho3 = rho3, rho4 = rho4) dados\u003c-data.frame(x,y,z,h) ggplot(dados, aes(x,y)) + geom_path(aes(color = 'Distribuição empírica')) + geom_path(aes(x,z, color = 'Normal'))+ geom_path(aes(x,h, color = 'Expansão Edgeworth')) Para n=100 library(EnvStats) n=100 m = 1000 obs = rlomax(m*n, scale = l, shape = a) # Distribuição empírica da soma estocástica de Lomax x = matrix(0, m, n) for(i in 1:m){ x[i,] = rlomax(n, scale = l, shape = a) } sn = apply(x, 1, sum) #Trabalhando com a soma estocástica padronizada empírico snp = (sn-n*mu)/(sqrt(n*sigma2)) x \u003c- qemp(p = seq(0, 1, len = 100), obs = snp) y \u003c- demp(x, snp) z \u003c- dnorm(x) h \u003c- edgeworth(x, n, rho3 = rho3, rho4 = rho4) dados\u003c-data.frame(x,y,z,h) ggplot(dados, aes(x,y)) + geom_path(aes(color = 'Distribuição empírica')) + geom_path(aes(x,z, color = 'Normal'))+ geom_path(aes(x,h, color = 'Expansão Edgeworth')) ","date":"18-03-2020","objectID":"/sexto-post/:5:2","tags":null,"title":"Expansões de Edgeworth para a distribuição Lomax","uri":"/sexto-post/"},{"categories":null,"content":"Um vírus que foi relatado pela primeira vez na cidade chinesa de Wuhan, agora se espalhou para mais de uma dúzia de países em todo o mundo, provocando uma crise econômica e de saúde sem precedentes. A Organização Mundial da Saúde (OMS) declarou o surto do coronavírus uma emergência de saúde pública de interesse mundial. Vamos dá uma breve olhada na crise atual e depois nos aprofundaremos no “Novel Corona Virus 2019 Dataset” do Kaggle. ","date":"17-02-2020","objectID":"/primeiro-post/:0:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"O que é um coronavírus? Segundo a OMS, os coronavírus (CoV) são uma grande família de vírus que causam doenças que variam do resfriado comum a doenças mais graves, como a Síndrome Respiratória do Oriente Médio (MERS-CoV) e a Síndrome Respiratória Aguda Grave (SARS-CoV). Um novo coronavírus (nCoV) é uma nova cepa que não foi previamente identificada em humanos. O vírus identificado como a causa do surto recente está sendo chamado de coronavírus 2019-nCoV ou Wuhan. ","date":"17-02-2020","objectID":"/primeiro-post/:1:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"A crise, a partir de hoje De acordo com o último relatório do New York Times, “o número de infecções confirmadas subiu para 37.198 e o número de mortos na China aumentou para 811, superando o número de mortos pela epidemia de SARS.” Dezesseis cidades na China, com uma população combinada de mais de 50 milhões de pessoas, estão confinadas. As companhias aéreas de todo o mundo cancelaram voos de ida e volta dá China. Alguns países estão evacuando seus cidadãos em vôos especiais e os colocando em quarentena rigorosa (Brasil por exemplo). Para piorar, as bolsas de valores caíram na China e os mercados em todo o mundo estão sentindo os efeitos. Alguns analistas prevêem que o surto representa uma ameaça para a economia global, e que tem o potencial de desencadear conseqüências geopolíticas de longo alcance. ","date":"17-02-2020","objectID":"/primeiro-post/:2:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"Uma introdução ao conjunto de dados O Conjunto de dados do “Novel Corona Virus 2019”, publicado no Kaggle, foi coletado pela John Hopkins University. A equipe coletou os dados de várias fontes, como a OMS, CDC local e meios de comunicação. Eles também criaram um painel em tempo real para monitorar a propagação do vírus. ","date":"17-02-2020","objectID":"/primeiro-post/:3:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"Importando e Carregando os Dados library(dplyr) dados\u003c-read.table('cv.csv', sep = ',', header = T) ","date":"17-02-2020","objectID":"/primeiro-post/:4:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"Compreendendo o conjunto de dados Vamos primeiro obter um entendimento básico do conjunto de dados e executar operações de limpeza de dados, se necessário. dados %\u003e% head() ## Sno Date Province.State Country Last.Update Confirmed ## 1 1 01/22/2020 12:00:00 Anhui China 01/22/2020 12:00:00 1 ## 2 2 01/22/2020 12:00:00 Beijing China 01/22/2020 12:00:00 14 ## 3 3 01/22/2020 12:00:00 Chongqing China 01/22/2020 12:00:00 6 ## 4 4 01/22/2020 12:00:00 Fujian China 01/22/2020 12:00:00 1 ## 5 5 01/22/2020 12:00:00 Gansu China 01/22/2020 12:00:00 0 ## 6 6 01/22/2020 12:00:00 Guangdong China 01/22/2020 12:00:00 26 ## Deaths Recovered ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 Os nomes das colunas são auto-explicativos. A primeira coluna ‘Sno’ se parece com um número de linha e não agrega valor à análise. A quinta coluna ‘Última atualização’ mostra o mesmo valor que a coluna ‘Data’, exceto em alguns casos em que os números foram atualizados posteriormente. Então, irei remover essas duas colunas antes de continuar. dados\u003c-dados[,c(-1,-5)] dados %\u003e% head() ## Date Province.State Country Confirmed Deaths Recovered ## 1 01/22/2020 12:00:00 Anhui China 1 0 0 ## 2 01/22/2020 12:00:00 Beijing China 14 0 0 ## 3 01/22/2020 12:00:00 Chongqing China 6 0 0 ## 4 01/22/2020 12:00:00 Fujian China 1 0 0 ## 5 01/22/2020 12:00:00 Gansu China 0 0 0 ## 6 01/22/2020 12:00:00 Guangdong China 26 0 0 Se fizer uma análise mais aprofundada, a base de dados mostra que faltam nomes de províncias para países como Reino Unido, França e Índia. Nesse caso, não podemos assumir ou preencher valores ausentes de nenhuma observação. dados %\u003e% summary() ## Date Province.State Country ## 02/14/2020 22:00:00: 75 : 418 Mainland China:739 ## 02/15/2020 22:00:00: 75 Anhui : 25 US :166 ## 02/13/2020 21:15:00: 74 Beijing : 25 Australia : 76 ## 02/11/2020 20:44:00: 73 Chongqing: 25 Canada : 53 ## 02/12/2020 22:00:00: 73 Fujian : 25 China : 34 ## 02/07/2020 20:24:00: 72 Gansu : 25 Japan : 25 ## (Other) :1127 (Other) :1026 (Other) :476 ## Confirmed Deaths Recovered ## Min. : 0 Min. : 0.000 Min. : 0.00 ## 1st Qu.: 2 1st Qu.: 0.000 1st Qu.: 0.00 ## Median : 12 Median : 0.000 Median : 0.00 ## Mean : 406 Mean : 9.121 Mean : 33.66 ## 3rd Qu.: 101 3rd Qu.: 0.000 3rd Qu.: 5.00 ## Max. :56249 Max. :1596.000 Max. :5623.00 ## Fazendo uma rápida análise descritiva, usando a função summary() A função summary() retorna as estatísticas gerais das colunas da base de dados. Uma conclusão imediata da saída é que os dados foram relatados cumulativamente, ou seja, o número de casos relatados em um determinado dia inclui os casos relatados anteriormente. O valor ‘máximo’ de mortes é 1596, o que é consistente com os relatos da mídia há alguns dias (quando esses dados foram publicados 15/02/2020). dados[,3] %\u003e% unique() ## [1] China US Japan ## [4] Thailand South Korea Mainland China ## [7] Hong Kong Macau Taiwan ## [10] Singapore Philippines Malaysia ## [13] Vietnam Australia Mexico ## [16] Brazil France Nepal ## [19] Canada Cambodia Sri Lanka ## [22] Ivory Coast Germany Finland ## [25] United Arab Emirates India Italy ## [28] Sweden Russia Spain ## [31] UK Belgium Others ## [34] Egypt ## 34 Levels: Australia Belgium Brazil Cambodia Canada China Egypt ... Vietnam Os dados mostram que o vírus se espalhou para 33 países da Ásia, Europa e América. Para facilitar essa análise, podemos mesclar dados para ‘China’ e ‘Mainland China’. (Others não é um país e sim a contagem dos valores em brancos). dados[which(dados[,3]=='Mainland China'),3]\u003c-'China' dados[,3] %\u003e% unique() ## [1] China US Japan ## [4] Thailand South Korea Hong Kong ## [7] Macau Taiwan Singapore ## [10] Philippines Malaysia Vietnam ## [13] Australia Mexico Brazil ## [16] France Nepal Canada ## [19] Cambodia Sri Lanka Ivory Coast ## [22] Germany Finland United Arab Emirates ## [25] India Italy Sweden ## [28] Russia Spain UK ## [31] Belgium Others Egypt ## 34 Levels: Australia Belgium Brazil Cambodia Canada China Egypt ... Vietnam Antes de av","date":"17-02-2020","objectID":"/primeiro-post/:5:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"Plotando os dados Para visualização dos dados, usaremos três poderosos pacotes do R: ggplot2, dplyr e tidyr. 1 - Número de casos confirmados ao longo do tempo library(ggplot2) ggplot(dados, aes(Date, Confirmed)) + geom_col(fill = \"#ff574d\") + scale_x_date(breaks = unique(dados[,1])) + theme(axis.text.x.bottom = element_text(angle = 90, hjust = 0)) Pelo gráfico acima, conseguimos ver mais de 60000 casos confirmados. 1.2 - Número de mortes confirmados ao longo do tempo ggplot(dados, aes(Date, Deaths)) + geom_col(fill = \"#008000\") + scale_x_date(breaks = unique(dados[,1])) + theme(axis.text.x.bottom = element_text(angle = 90, hjust = 0)) Pelo gráfico acima, conseguimos ver mais de 1500 mortes. 1.3 - Número de casos recuperado ao longo do tempo ggplot(dados, aes(Date, Recovered)) + geom_col(fill = \"#5390fe\") + scale_x_date(breaks = unique(dados[,1])) + theme(axis.text.x.bottom = element_text(angle = 90, hjust = 0)) Pelo gráfico acima, conseguimos ver mais de 7500 casos de pessoas recuperadas. 1.4 - Todos os casos ao longo do tempo library(tidyr) dados %\u003e% gather(Casos, Quantidade, -Province.State, -Date,-Country) %\u003e% ggplot(aes(Date, Quantidade, fill=Casos)) + geom_bar(stat = \"identity\", position = \"dodge\") + scale_x_date(breaks = unique(dados[,1])) + theme(axis.text.x.bottom = element_text(angle = 90, hjust = 0)) Mortes e Recuperados Diante todo o caos, podemos retirar uma noticia boa. Os casos de pessoas recuperadas já superam e muito o número de mortos. library(tidyr) mxr\u003c-dados %\u003e% gather(Casos, Quantidade, -Province.State, -Date,-Country) mxr\u003c-mxr[mxr$Casos!='Confirmed',] mxr%\u003e% ggplot(aes(Date, Quantidade, fill=Casos)) + geom_bar(stat = \"identity\", position = \"dodge\") + scale_x_date(breaks = unique(dados[,1])) + theme(axis.text.x.bottom = element_text(angle = 90, hjust = 0)) Um olhar mais atento às 10 províncias mais afetadas da China dados_china\u003c-dados[which(dados[,3]=='China'),] dados_china %\u003e%group_by(Province.State) %\u003e% summarise(Confirmed=max(Confirmed), Deaths=max(Deaths), Recovered = max(Recovered))%\u003e% arrange(desc(Confirmed)) ## # A tibble: 34 x 4 ## Province.State Confirmed Deaths Recovered ## \u003cfct\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e ## 1 Hubei 56249 1596 5623 ## 2 Guangdong 1294 2 410 ## 3 Henan 1212 13 391 ## 4 Zhejiang 1162 0 428 ## 5 Hunan 1001 2 425 ## 6 Anhui 950 6 221 ## 7 Jiangxi 913 1 210 ## 8 Jiangsu 604 0 186 ## 9 Chongqing 544 5 184 ## 10 Shandong 532 2 156 ## # … with 24 more rows Observações A província chinesa de Hubei é o epicentro do surto. Tem significativamente mais casos relatados do que todas as outras províncias juntas. Existem algumas províncias onde não houve mortes e todos os pacientes afetados se recuperaram. O número de casos relatados diariamente aumentou quase 500% desde 22 de janeiro. Isso mostra que o vírus é altamente contagioso e está se espalhando rapidamente. Durante a primeira semana, a taxa de mortes foi superior à das recuperações. Desde 31 de janeiro, a taxa de recuperação disparou e está mostrando uma tendência positiva. Houve 255 recuperações no dia 4 de fevereiro, em comparação com 66 mortes. A taxa de recuperação continuará a aumentar à medida que mais pessoas conhecerem os sintomas e forem rápidas na procura de medicamentos. Países geograficamente próximos à China, como Tailândia, Japão e Cingapura, relataram mais casos do que outros países asiáticos e europeus. A Alemanha é uma exceção e tem o maior número de casos na Europa. ","date":"17-02-2020","objectID":"/primeiro-post/:6:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"Conclusão A análise mostra a taxa alarmante na qual o coronavírus está se espalhando. Pelo menos 1590 pessoas já morreram durante a atual epidemia, excedendo as 774 mortes relatadas durante o surto de SARS há sete anos. ","date":"17-02-2020","objectID":"/primeiro-post/:7:0","tags":null,"title":"Coronavírus no mundo.","uri":"/primeiro-post/"},{"categories":null,"content":"Na minha graduação fiz vários relatórios, das mais diversas disciplinas. Aqui, irei apresentar um resumo de um desses relatórios. Esse Relatório foi feito na disciplina de MLG, ministrado pela Prof. Dra. Michelli Karinne Barros da Silva. ","date":"10-02-2020","objectID":"/terceiro-post/:0:0","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"Introdução O objetivo desse trabalho é saber de que forma os níveis de voltagem e temperatura afetam o tempo médio de resistência dos vidros. Primeiramente foi feito uma análise descritiva dos dados, mostrando a influencia dos dados sobre a variável resposta. E logo após, o ajuste do modelo para mostrar que as variáveis voltagem e temperatura interferem na resistência dos vidros. ","date":"10-02-2020","objectID":"/terceiro-post/:1:0","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"Análise de Dados Nas tabelas abaixo são apresentados os resultados de um experimento, em que a resistência (em horas) de um determinado tipo de vidro, foi avaliada segundo quatro níveis de voltagem (em kilovolts) e duas temperaturas (em graus Celsius). ","date":"10-02-2020","objectID":"/terceiro-post/:2:0","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"Dados do Experimento Voltagens com temperatura de 170c 200kV 250kV 300kV 350kV 439h 572h 315h 258h 904h 690h 315h 258h 1092h 904h 439h 347h 1105h 1090h 628h 588h Voltagens com temperatura de 180c 200kV 250kV 300kV 350kV 959h 216h 241h 241h 1065h 315h 315h 241h 1065h 455h 332h 435h 1087h 473h 380h 455h ","date":"10-02-2020","objectID":"/terceiro-post/:2:1","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"Análise Descritiva Na tabela abaixo são apresentadas as médias, desvios e coeficiente de variação das variáveis Resistência, voltagem e temperatura. Podemos perceber que a resistência média foi de 569,34 ao passo que a voltagem e a temperatura apresentaram médias iguais a 275 e 175, respectivamente. O coeficiente de variação da resistência é próximo de 56%, indicando assim, uma alta dispersão nos dados. Algumas estatísticas para cada variável Resistência Voltagem Temperatura Média 569,34 275 175 Desvio 316,98 56,80 5,08 CV 55,67% 20,65 % 2.90 % Nas tabelas abaixo, fixando cada uma das co-variáveis, nota-se valores maiores para a resistência média com a menor temperatura (170c) e menor voltagem (200kV). Todavia, o coeficiente de variação apesar de diminuir quando fixa-se as co-variáveis, apresenta uma dispersão média dos dados. Resistencia fixando a temperatura Resistência à 170c Resistência à 180c Média 621,50 517,19 Desvio 309,47 325,67 CV 49,80% 62,97% Resistencia fixando as voltagens Resistência à 200kv Resistência à 250kv Resistência à 300kv Resistência à 350kv Média 964,50 589,38 370,63 352,88 Desvio 223,94 294,31 118,67 128,47 CV 23,21% 49,93% 32,01% 36,40% Percebe-se que as maiores médias da resistência, foram com menor voltagem 200kv e temperatura 170c. É possível ver também esses dois fatores acontecendo no gráfico abaixo. Nota-se duas observações discrepantes nos dados, uma com voltagem a 200kV e a outra a 300kV. Ainda é possível ver a voltagem e temperatura interagindo sobre a resistência do vidro. Como é mostrado no gráfico abaixo Ainda, podemos fixar as duas variáveis voltagem e temperatura e ver as estatísticas descritivas para cada uma delas. Algumas estatísticas para a resistência fixando a temperatura a 170c e variando a voltagem Resistência à 200kv e 170c Resistência à 250kv e 170c Resistência à 300kv e 170c Resistência à 350kv e 170c Média 885,00 814,00 424,25 362,75 Desvio 311,20 229,65 158,88 155,92 CV 35,16 % 28,21% 34,85% 42,98 % Algumas estatísticas para a resistência fixando a temperatura a 180c e variando a voltagem Resistência à 200kv e 180c Resistência à 250kv e 180c Resistência à 300kv e 180c Resistência à 350kv e 180c Média 1,04 364,75 317,00 343,00 Desvio 5,76 121,74 57,66 118,06 CV 5,52% 33,38% 18,19% 34,42% O mesmo acontece com a análise anterior, a maior média da resistência se da pela temperatura a 170c e voltagem 200kV, sendo ela 885,00. Já no gráfico de interação, indica-se uma redução da resistência média à medida que aumenta o nível de voltagem. Além disso, há indicação de interação entre voltagem e temperatura. ","date":"10-02-2020","objectID":"/terceiro-post/:3:0","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"Ajuste do Modelo Para fazer o ajuste do modelo precisamos saber qual distribuição melhor se encaixa com os dados. Para isso o gráfico abaixo mostra se existe algum tipo de simetria. Como o interesse principal desse estudo é comparar as resistências médias, é usual neste tipo de estudo, assumir respostas com alguma distribuição assimétrica. Isso foi comprovado na figura acima, que mostrou possuir assimetria a esquerda. Assim, vamos propor que a variável resposta tenha distribuição gama. Foram considerado dois ajustes, sem e com interação da voltagem e temperatura. Na tabela abaixo são apresentados as estimativas do ajuste sem a interação. Todas as estimativas foram bastante significativas. Variável Estimativas P-valor Intercepto 1039,94 0.00 Voltagem (250kV) -426,49 0.00 Voltagem (300kV) -608,81 0.00 Voltagem (350kV) -612,89 0.00 Temperatura (180c) -117,78 0.05 phi 9.49 (2,33) - No ajuste com interação, algumas variáveis da interação foram significativas a 5%. Variável Estimativas P-valor Intercepto 885,00 0,00 Voltagem (250kV) -71,00 0,71 Voltagem (300kV) -460,75 0,01 Voltagem (350kV) -522,25 0,00 Temperatura (180c) 159,00 0,46 Voltagem (250kV) : Temperatura (180c) -608,25 0,03 Voltagem (300kV) : Temperatura (180c) -266,25 0,26 Voltagem (350kV) : Temperatura (180c) -178,75 0,44 phi 13,35 (3,30) - Para decidir qual ajuste usar, foi feito um teste F utilizando os dois ajustes. As hipóteses do teste F são: $H_0$: Teste sem interação; $H_1$: Teste com interação As estimativas do teste F foram: Teste F P-valor 3.45 0.0325 No qual, foi mostrado que o ajuste com interação é mais apropriado ao nível de significância a 5%. Porém a 1% o mesmo deixa de ser significativo, nos mostrando que não a diferença de escolha. Inicialmente foi feito uma seleção de variáveis através do método AKAIKE. Contudo, nenhuma variável deixou de ser significativa, nos mostrando que a resistência depende da voltagem e temperatura. Na Figura abaixo são apresentados alguns gráficos de diagnóstico, notamos uma discrepância da observação 16, que aparece como ponto influente e aberrante. Além disso, notamos indícios de heteroscedasticidade, ou seja, uma diminuição da variabilidade com o aumento do valor ajustado. Assim, podemos propor um modelo alternativo, que ajuste melhor a variância que o modelo Gamma. A eliminação desses pontos muda as estimativas, embora não mude a inferência. As novas estimativas são: Estimativas sem a obs. 1 Estimativas sem a obs. 16 Estimativas sem as obs. 1 e 16 Intercepto 1033,67 885,00 1033,67 Voltagem (250kV) -219,67 -71,00 -219,67 Voltagem (300kV) -609,42 -460,80 -609,42 Voltagem (350kV) -670,92 -597,30 -746,00 Temperatura (180c) 10,33 159,00 10,33 Voltagem (250kV) e Temperatura (180c) -459,58 -608,20 -459,58 Voltagem (300kV) e Temperatura (180c) -117,58 -266,20 -117,58 Voltagem (350kV) e Temperatura (180c) -30,08 -103,70 45,00 phi 16,00 (4,023) 15,65 (3,93) 19,75 (5,06) Ao verificar o impacto que houve na retirada dessas obs. Temos : Impacto sem a obs. 1 Impacto sem a obs. 16 Impacto sem as obs.1 e 16 Intercepto -16,80 0,00 -16,80 Voltagem (250kV) -209,39 0,00 -209,39 Voltagem (300kV) -32,27 0,00 -32,27 Voltagem (350kV) -28,47 -14,38 -42,84 Temperatura (180c) 93,50 0,00 93,50 Voltagem (250kV) : Temperatura (180c) 24,44 0,00 24,44 Voltagem (300kV) : Temperatura (180c) 55,84 0,00 55,84 Voltagem (350kV) : Temperatura (180c) 83,17 42,01 125,18 O impacto é bastante alto, dado que o valor esperado para a retirada das duas observações seria de 6,25. Isso acontece pois a adequação do modelo e a existência de observações atípicas estão influenciando diretamente no ajuste. Essa adequação pode ser vista na figura abaixo Não observamos pontos muito fora do envelope, porém observamos pontos muito fora do alinhamento. Por conseguinte, há indicação de observações atípicas, e que o modelo seja inadequado. ","date":"10-02-2020","objectID":"/terceiro-post/:4:0","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"Conclusão O que foi visto na análise descritiva se refletiu no ajuste do modelo, as variáveis voltagem e temperatura são extremamente significativas para a resistência do vidro. Mesmo o ajuste Gamma não sendo o melhor. A voltagem 350kV diminui cerca de 522,25 a resistência do vidro, já a temperatura $180$c aumenta a resistência do vidro em 159,00. Porem, quando fazemos a interação das duas variáveis que é o que realmente interessa, ja que a produção de vidro depende dessas duas variáveis, temos que a voltagem 250kV e a temperatura 180c diminui cerca de 608,25 a resistência do vidro em comparação a voltagem 200kV e a temperatura 170c. O que podemos afirmar, segundo os dados, que, para se obter uma maior resistência dos vidros, devemos manter em temperatura a 170c e a voltagem 200kV. ","date":"10-02-2020","objectID":"/terceiro-post/:5:0","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"R library(fBasics) library(ggplot2) library(MASS) dados\u003c-read.table('Documentos/UFCG/2018.2/MLG/ANALISES/7/vidros.dat') names(dados)\u003c-c('tresistencia','voltagem','temperatura') dados$voltagem1\u003c-1:32 dados$voltagem1[which(dados$voltagem==1)] \u003c-200 dados$voltagem1[which(dados$voltagem==2)] \u003c- 250 dados$voltagem1[which(dados$voltagem==3)] \u003c- 300 dados$voltagem1[which(dados$voltagem==4)] \u003c- 350 dados$temperatura1\u003c-1:32 dados$temperatura1[which(dados$temperatura==1)]\u003c-170 dados$temperatura1[which(dados$temperatura==2)]\u003c-180 dados$temperatura\u003c-factor(dados$temperatura) dados$voltagem\u003c-factor(dados$voltagem) attach(dados) (mgr\u003c-mean(tresistencia)) (mgv\u003c-mean(voltagem1)) (mgt\u003c-mean(temperatura1)) 100*(cv1\u003c-sd(tresistencia)/mean(tresistencia)) 100*(cv2\u003c-sd(voltagem1)/mean(voltagem1)) 100*(cv3\u003c-sd(temperatura1)/mean(temperatura1)) (media_fixo_temperatura170\u003c-sd(split(tresistencia,temperatura1)$'170'))/(media_fixo_temperatura170\u003c-mean(split(tresistencia,temperatura1)$'170')) (media_fixo_temperatura180\u003c-sd(split(tresistencia,temperatura1)$'180'))/(media_fixo_temperatura180\u003c-mean(split(tresistencia,temperatura1)$'180')) (media_fixo_voltagem1\u003c-mean(split(tresistencia,voltagem1)$'200')) (media_fixo_voltagem2\u003c-mean(split(tresistencia,voltagem1)$'250')) (media_fixo_voltagem3\u003c-mean(split(tresistencia,voltagem1)$'300')) (media_fixo_voltagem4\u003c-mean(split(tresistencia,voltagem1)$'350')) sd(split(tresistencia,voltagem1)$'200')/mean(split(tresistencia,voltagem1)$'200') sd(split(tresistencia,voltagem1)$'250')/mean(split(tresistencia,voltagem1)$'250') sd(split(tresistencia,voltagem1)$'300')/mean(split(tresistencia,voltagem1)$'300') sd(split(tresistencia,voltagem1)$'350')/mean(split(tresistencia,voltagem1)$'350') dados\u003c-as.data.frame(dados) par(mfrow=c(1,2)) plot(factor(voltagem1),tresistencia, xlab=('Voltagem'),ylab=('Resistencia'), col=5) plot(factor(temperatura1),tresistencia, xlab=('Temperatura'),ylab=('Resistencia'),col=7) ggplot(dados,aes(y=tresistencia,x=voltagem1, fill = factor(temperatura1)))+ geom_col() + labs(x = \"Voltagem kV\",y = \"Tempo de Resistência\")+ theme_light() ggplot(dados,aes(y=tresistencia,x=voltagem1, colour = factor(temperatura1)))+ geom_point(size =5, shape =18) + labs(x = \"Voltagem kV\",y = \"Tempo de Resistência\")+ theme_light() ggplot(dados, aes(x=tresistencia)) + geom_density(fill=5) + xlim(0, 1800)+ theme_light() modelo\u003c-glm(tresistencia~voltagem + temperatura,family = Gamma(link=identity)) summary(modelo) xtable(summary(modelo)) with(dados, interaction.plot(voltagem, temperatura, tresistencia, xlab = \"Voltagem\", ylab = \"Resistência\", col=4)) summary(modelo) gamma.shape(modelo) (modelo1\u003c-glm(tresistencia~voltagem + temperatura + voltagem*temperatura, family = Gamma(link=identity))) summary(modelo1) gamma.shape(modelo1) xtable(anova(modelo,modelo1, test = 'F')) library(MASS) (modelo2\u003c-stepAIC(modelo1)) summary(modelo2) gamma.shape(modelo2) fit.model\u003c-modelo2 X \u003c- model.matrix(fit.model) n \u003c- nrow(X) p \u003c- ncol(X) w \u003c- fit.model$weights W \u003c- diag(w) H \u003c- solve(t(X)%*%W%*%X) H \u003c- sqrt(W)%*%X%*%H%*%t(X)%*%sqrt(W) h \u003c- diag(H) library(MASS) fi \u003c- gamma.shape(fit.model)$alpha ts \u003c- resid(fit.model,type=\"pearson\")*sqrt(fi/(1-h)) td \u003c- resid(fit.model,type=\"deviance\")*sqrt(fi/(1-h)) di \u003c- (h/(1-h))*(ts^2) a \u003c- max(td) b \u003c- min(td) plot(fitted(fit.model),h,xlab=\"Valor Ajustado\", ylab=\"Medida h\", pch=16,ylim=c(0,0.5)) #identify(fitted(fit.model), h, n=1) # par(mfrow=c(1,2)) plot(di,xlab=\"Índice\", ylab=\"Distância de Cook\", pch=16) identify(di, n=1) # plot(fitted(fit.model),td,xlab=\"Valor Ajustado\", ylab=\"Resíduo Componente do Desvio\", ylim=c(b-1,a+1),pch=16) abline(2,0,lty=2) abline(-2,0,lty=2) identify(fitted(fit.model),td, n=2) # w \u003c- fit.model$weights eta \u003c- predict(fit.model) z \u003c- eta + resid(fit.model, type=\"pearson\")/sqrt(w) plot(predict(fit.model),z,xlab=\"Preditor Linear\", ylab=\"Variável z\", pch=16) #------------------------------------------------------------# par(mfrow=c(1,1)) X \u003c- model.matrix(fit.model) n \u003c- nrow(X) p \u003c- ncol","date":"10-02-2020","objectID":"/terceiro-post/:6:0","tags":null,"title":"Resistência dos vidros (voltagem x temperatura) ","uri":"/terceiro-post/"},{"categories":null,"content":"Este trabalho foi apresentado no Conapesc 2019, e teve orientação da Prof. Dr. Amanda dos Santos Gomes, amanda.natalia.gomes@gmail.com. Temos como objetivo mostrar o desempenho dos mais populares testes de normalidade para dados com e sem a presença de outliers e verificar o nível de concordância dos testes nestes tipos de situações. De maneira geral, verificou-se que o teste de Jarque-Bera é o mais poderoso nas amostras com a presença de outliers. O teste teve o melhor desempenho para pequenas e grandes amostras. Nas amostras sem a presença de outliers, o teste de Shapiro-Wilk foi o mais poderoso seguido do Shapiro-Francia. Aplicou-se o coeficiente Kappa-Fleiss para avaliar a concordância na tomada de decisão. Para amostras com outliers os testes apresentaram baixa concordância e para amostras sem outliers os testes apresentaram forte concordância. ","date":"20-02-2019","objectID":"/nove-post/:0:0","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Introdução No dia a dia das pessoas que trabalham com dados amostrais ou experimentais é bastante comum a suposição de normalidade dos dados, isso porque é uma condição exigida para a realização de muitas inferências válidas a respeito de parâmetros populacionais. A distribuição normal é um modelo probabilístico contínuo, sendo um dos mais importantes, pois a maioria dos métodos estatísticos são baseados nesse modelo e muitos fenômenos aleatórios podem ser descritos de forma aproximada por ele. Na inferência há vários pressupostos a serem observados, os pressupostos estatísticos mais considerados são normalidade, linearidade e homoscedasticidade. Este trabalho focará apenas no pressuposto de normalidade visto que exige-se por inúmeros procedimentos estatísticos, tais como: construção de intervalos de confiança, testes de hipóteses, análise de variância e modelagem estatística. Assim, torna-se importante verificar esse pressuposto antes de se prosseguir com os procedimentos estatísticos que o exijam. O software R disponibiliza diversas maneiras de verificar a normalidade dos dados, sendo eles, métodos gráficos, métodos numéricos e os testes de normalidade. O mais famoso método gráfico é o QQ-plot, no qual o mesmo imprime os pontos sobre uma reta afim de avaliar sua normalidade, sendo porém uma técnica subjetiva. Assim, para se obter uma conclusão formal é importante efetuar um teste estatístico. A motivação para este trabalho surgiu a partir do interesse de avaliar o desempenho dos testes de normalidade de Shapiro-Wilk, Anderson-Darling, Lilliefors, Jarque-Bera, Shapiro-Francia e Cramer-von Mises para dois tipos situações: A primeira para dados sem a presença de outliers e a segunda com a presença de outliers. O desempenho será realizado a partir das análises da taxas de erro tipo I e poder empírico. ","date":"20-02-2019","objectID":"/nove-post/:1:0","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Metodologia Neste trabalho foram consideradas algumas estratégias para avaliar as taxas do erro do tipo I, poder e concordância entre esses testes. Foi utilizada a simulação de Monte Carlo, e em cada simulação foram aplicados os testes de normalidade em um nível de significância pré-estabelecido de 5%, sendo verificado se a hipótese nula, de que os dados seguem uma distribuição normal, foi ou não rejeitada. Caso tenha sido rejeitada a hipótese nula e a amostra tenha sido gerada da distribuição normal, foi cometido um erro do tipo I. Da mesma forma se a hipótese nula for rejeitada e a amostra foi obtida de uma população não-normal, uma decisão correta foi tomada. Em cada caso, foi repetido 10.000 vezes e a proporção de decisões incorretas no primeiro caso é a taxa de erro tipo I e no segundo caso, a proporção de decisões corretas é o poder empírico do teste. Os testes de Anderson-Darling, Lilliefors, Shapiro-Francia e Cramer-von Mises foram aplicados utilizando as funções, ad.test(), lilli.test(), sf.test() e cvm.test(), respectivamente. Todas essas funções são encontradas no pacote nortest. O teste de Shapiro-Wilk foi aplicado utilizando-se a função shapiro.test() do pacote stats, e o teste Jarque-Bera com função jb.norm.test() do pacote normtest. ","date":"20-02-2019","objectID":"/nove-post/:2:0","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Desenvolvimento ","date":"20-02-2019","objectID":"/nove-post/:3:0","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Simulação Monte Carlo Foram realizados dois experimentos, o primeiro com 10.000 replicas de Monte Carlo sob as hipóteses $H_0$ e $H_1$, de diversas distribuições de probabilidade, em que foi produzido de cada distribuição, amostras de tamanho n, nos quais esses valores foram: 10, 20, 30, 50, 100, 200, 300, 500 e 1.000. E no segundo experimento foi adotada a mesma ideia do primeiro, porém, todas as distribuições geradas continham outliers. ","date":"20-02-2019","objectID":"/nove-post/:3:1","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Erro tipo I Foram simuladas 10.000 amostras aleatórias normais de tamanhos $n$ com a função rnorm, com média 0 e desvio padrão 1. Se, ao aplicar os testes, a hipótese nula de normalidade é rejeitada a um nível de significância de 5%, então a distribuição dos dados é considerada erroneamente como não-normal. A proporção de rejeições incorretas foi calculada para cada teste e representam as taxas de erro tipo I. ","date":"20-02-2019","objectID":"/nove-post/:3:2","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Poder Foram simuladas amostras aleatórias de distribuições não-normal para avaliar o poder dos testes, que é rejeitar a hipótese nula que por construção é falsa. Em dois experimentos, 10.000 amostras de tamanho $n$ de algumas distribuições não-normais, simétricas e assimétricas, foram simuladas. No primeiro sem a presença de outliers, e no segundo com a presença de outliers. As distribuições simétricas foram a uniforme(0,1), beta(2,2) e t-student(10). Já as distribuições assimétricas foram Gamma(2,1), $\\chi^2$(15), lognormal(0,1), exponencial(1) e weibull(2,1). As distribuições foram simuladas por meio das funções random do software R. Os testes de normalidade foram aplicados em cada uma das 10.000 amostras de cada distribuição simulada e a proporção de rejeições corretas da hipótese nula foi computada. Os valores obtidos representam o poder dos testes que foram comparados entre si. ","date":"20-02-2019","objectID":"/nove-post/:3:3","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Concordância Os dados foram computados para cada uma das distribuições com n tamanhos diferentes. Foi feito um coeficiente Kappa-Fleiss com função Kappam.fleiss do pacote irr, para verificar o nível concordância entre os testes na tomada de decisão entre rejeitar ou não a hipótese nula de normalidade. A concordância foi aplicado apenas nas amostras não provenientes da distribuição normal. Com isso, verificaremos se o poder dos testes interfere diretamente na concordância. ","date":"20-02-2019","objectID":"/nove-post/:3:4","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Resultados e Discussões Nesta seção serão apresentados os resultados das simulações para obtenção do erro do tipo I, poder e concordância dos testes de normalidade. ","date":"20-02-2019","objectID":"/nove-post/:4:0","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Sem a Presença de Outliers Foram avaliados seis testes de normalidade, Anderson-Darling, Lilliefors, Shapiro-Francia, Cramer-von Mises, Shapiro-Wilk e Jarque-Bera. A Tabela [1] apresenta as taxas de erro tipo I para diferentes tamanhos amostrais. Tabela 1: Erro do tipo I das amostras sem outliers dos testes Anderson-Darling (AD), Lilliefors (LL), Shapiro-Francia (SF), Cramer-von Mises (CVM), Shapiro-Wilk (SW) e Jarque- Bera (JB) Tamanho SW AD CVM LL SF JB 10 0,0466 0,0442 0,0424 0,0478 0,0506 0,0484 15 0,0482 0,0502 0,0496 0,0472 0,0528 0,0548 20 0,0496 0,0500 0,0500 0,0506 0,0496 0,0476 30 0,0544 0,0520 0,0514 0,0470 0,0514 0,0514 50 0,0460 0,0512 0,0542 0,0546 0,0482 0,0456 100 0,0512 0,0506 0,0498 0,0530 0,0520 0,0532 200 0,0528 0,0524 0,0530 0,0484 0,0524 0,0480 300 0,0478 0,0480 0,0434 0,0454 0,0488 0,0484 500 0,0514 0,0450 0,0482 0,0472 0,0504 0,0474 1000 0,0496 0,0468 0,0460 0,0470 0,0520 0,0500 As duas Figuras abaixo são a representação da Tabela 1. Por meio das duas Figuras acima e da Tabela 1 verificou-se que a taxa de erro do tipo I, para todos os testes, oscilou consideravelmente em torno do valor nominal de 5%, para amostras pequenas, $n\\leq 30$. Ainda pela Figura 1, nota-se que a medida que o tamanho da amostra aumenta, os testes de Shapiro-Francia e Shapiro-Wilk vão se aproximando mais rápido da taxa nominal de 0,05 tornando-os assim testes mais precisos. O teste Jarque-Bera tende a rejeitar menos à hipótese nula de normalidade, pois a maioria das suas oscilações estão abaixo do nível nominal de 0,05. O teste de Shapiro-Francia pouco oscila sobre o valor nominal, o tornando um dos testes mais precisos para calcular o erro do tipo I. Os testes de Anderson-Darling, Lilliefors e Cramer-von Mises, são os menos precisos, ficando distantes do valor nominal. Já o teste de Shapiro-Wilk, é mais poderoso entre eles tendo baixas oscilações em torno do valor nominal. Pela Tabela 2 podemos observar a variância dos testes em relação ao valor nominal para todas as amostras. Nas Tabelas 3 e 4 podemos observar para pequenas e grandes amostras, respectivamente. Tabela 2: Variância para todos tamanhos amostrais SW AD CVM LL SF JB $6,400 \\times 10^{-7}$ $1,024 \\times 10^{-5}$ $1,600 \\times 10^{-5}$ $1,547 \\times 10^{-5}$ $7,471 \\times 10^{-6}$ $3,004 \\times 10^{-6}$ Tabela 3: Variância para pequenas amostras $n \\leq 30$ SW AD CVM LL SF JB $4,800 \\times 10^{-7}$ $4,320 \\times 10^{-6}$ $1,452\\times 10^{-5}$ $1,825\\times 10^{-5}$ $6,453 \\times 10^{-6}$ $1,613\\times 10^{-6}$ Tabela 4: Variância para grandes amostras $n \\geq 50$ SW AD CVM LL SF JB $2,048 \\times 10^{-6}$ $3,200 \\times 10^{-6}$ $3,200 \\times 10^{-6}$ $1,095 \\times 10^{-5}$ $5,408 \\times 10^{-6}$ $7,200 \\times 10^{-6}$ Percebe-se que o teste de Shapiro-Wilk seguido do Shapiro-Francia foram os mais precisos de uma forma geral. Porém, para pequenas amostras o teste de Shapiro-Francia perde a segunda colocação para o teste Jarque-Bera, já que mesmo teve menor variância. Na Figura abaixo encontramos o poder de cada teste para as distribuições simétricas e assimétricas, com diferentes tamanhos amostrais. Para amostras provenientes da distribuição uniforme e beta, percebe-se que o teste de Shapiro-Willk tem um comportamento melhor quando se trata de amostras pequenas, seguido do teste de Anderson-Darling. Em relação aos demais testes, o teste Jarque-Bera apresentou o menor poder nas distribuições uniforme e beta para amostras pequenas, para amostras maiores que 150 o teste de Lilliefors apresentou menor poder. Para a distribuição t-student os testes diferem bastante quanto ao poder, mesmo para o maior tamanho amostral. A dificuldade dos testes em identificar corretamente a falta de normalidade quando os dados seguem a distribuição t-student se deve ao fato das duas distribuições possuírem densidades muito parecidas. Levando em consideração todos os tamanhos amostrais, o teste que melhor identificou os dados provenientes da distribuição t-student foi o Jarque-Bera, seguido do tes","date":"20-02-2019","objectID":"/nove-post/:4:1","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Com a presença de outliers A seguir serão apresentados os resultados das simulações com a presença de outliers para obtenção do erro do tipo I, poder e concordância dos testes de normalidade. Em cada amostra foram adicionados dois outliers, sendo um 10% do valor mínimo e o outro 100% do valor máximo. Tabela 6: Erro do tipo I das amostras com outliers dos testes Anderson-Darling (AD), Lilliefors (LL), Shapiro-Francia (SF), Cramer-von Mises (CVM), Shapiro-Wilk (SW) e Jarque- Bera (JB) Tamanho SW AD CVM LL SF JB 10 0,16 0,15 0,15 0,14 0,20 0,27 15 0,27 0,23 0,22 0,19 0,35 0,45 20 0,38 0,28 0,26 0,20 0,47 0,57 30 0,49 0,32 0,28 0,22 0,60 0,71 50 0,64 0,35 0,29 0,21 0,76 0,83 100 0,84 0,34 0,27 0,20 0,93 0,94 200 0,96 0,31 0,23 0,16 0,99 0,97 300 0,99 0,25 0,18 0,13 1,00 0,98 500 1,00 0,20 0,15 0,10 1,00 0,99 1000 1,00 0,13 0,10 0,08 1,00 0,99 De forma geral, todos os testes apresentaram dificuldades em identificar a normalidade dos dados com a presença de outliers. Pelas Figura abaixo, nota-se que os testes são sensíveis com a presença de outliers se distanciando bastante do valor nominal de 0,05. Os testes de Anderson-Darling, Lilliefors e Cramer-von Mises foram os menos sensíveis aos outliers, obtendo melhor desempenho se aproximando do valor nominal de acordo com a Tabela 6. Pela Tabela 7 podemos observar a variância dos testes em relação ao valor nominal para todas as amostras. Nas Tabelas 8 e 9 podemos observar para pequenas e grandes amostras. Tabela 7: Variância para todos tamanhos amostrais SW AD CVM LL SF JB 4,313 0,472 0,295 0,142 5,138 5,760 Tabela 8: Variância para pequenas amostras $n \\leq 30$ SW AD CVM LL SF JB 0,4033333 0,2028 0,1680333 0,1008333 0,6721333 1,08 Tabela 9: Variância para grandes amostras $n \\geq 50$ SW AD CVM LL SF JB 6,20498 0,4805 0,2645 0,1125 7,03298 7,34472 Percebe-se que os testes de Anderson-Darling, Cramer-von Mises e Lilliefors foram os mais precisos de uma forma geral. Para pequenas e grandes amostras o teste de Lilliefors ficou com a menor variância, seguido do teste de Cramer-von Mises. O teste Jarque-Bera foi o mais sensível de todos tendo uma alta variação. O poder para as distribuições simétricas e assimétricas geradas com a presença de outliers, pode ser observado pela Figura 6. Para amostras provenientes da distribuição uniforme, percebe-se que o teste Jarque-Bera tem o melhor comportamento quando se trata de amostras pequenas, seguido do teste de Shapiro-Francia. A partir de amostras maiores que 250, o teste mais poderoso foi o de Shapiro-Francia, seguido do Shapiro-Wilk. Os mesmos se tornam melhores opções, pois não há oscilações como o teste Jarque-Bera. Em relação aos demais testes, o teste de Lilliefors apresentou menor poder. Nas amostras provenientes da distribuição beta, o teste Jarque-Bera tem o melhor poder para amostras pequenas. O mesmo ganha poder rapidamente seguido do teste Shapiro-Francia. Os demais testes apresentaram oscilações a medida que as amostras aumentam, porém, essas oscilações tendem a ser crescentes tornando os testes mais poderosos. O teste de Lilliefors mais uma vez, foi o menos poderoso, mesmo para grandes amostras. Para amostras com outliers provenientes da distribuição t-student, os testes seguem com a mesma dificuldade apresentadas na seção anterior. A dificuldade dos testes em identificarem corretamente a falta de normalidade dos dados se deve ao fato das duas distribuições possuírem densidades muito parecidas. Em consideração a todos os tamanhos amostrais, o teste com melhor desempenho na distribuição t-student com a presença de outliers foi o Jarque-Bera, seguido do teste Shapiro-Francia. O teste Lilliefors apresentou o menor poder na distribuição t-student. Nas amostras provenientes das distribuições gamma, exponencial, lognormal e $\\chi^2$ os testes apresentaram ótimos resultados de poder, inclusive o teste de Lilliefors. Mesmo assim, os testes de Shapiro-Wilk e Shapiro-Francia foram superiores em todos os casos. Coeficiente Kappa para amostras c","date":"20-02-2019","objectID":"/nove-post/:4:2","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"Considerações finais Para dados de distribuição normal sem a presença de outliers, os testes avaliados foram bem precisos tendo pouca variação em torno do valor nominal de 5%, onde os melhores testes para a taxa de erro tipo I foram o Shapiro-Wilk seguido do Shapiro-Francia. Em relação aos dados de distribuição normal com a presença de outliers, os melhores testes para a taxa de erro tipo I foram o Lilliefors seguido do Cramer-von Mises. Vale ressaltar as dificuldades de todos os testes para amostras com a presença de outliers os mesmos tiveram alta variação em relação ao valor nominal. Em relação ao poder, os testes de Shapiro-Wilk seguido do Shapiro-Francia tiveram melhor desempenho para as distribuições geradas sem a presença de outliers, tornando-os testes mais poderosos/adequados para esse tipo de dados. Nas distribuições geradas com a presença de outliers, o teste Jarque-Bera foi o que obteve melhor desempenho seguido do teste de Shapiro-Wilk. ","date":"20-02-2019","objectID":"/nove-post/:5:0","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"},{"categories":null,"content":"R library(nortest) library(normtest) library(moments) alpha=0.05 m\u003c-10000 shap\u003c-NULL and\u003c-NULL cvm\u003c-NULL lilli\u003c-NULL sf\u003c-NULL jb\u003c-NULL for(i in 1:m){ amostra \u003c- rbeta(10,2,2) a\u003c-shapiro.test(amostra) b\u003c-ad.test(amostra) c\u003c-cvm.test(amostra) d\u003c-lillie.test(amostra) e\u003c-sf.test(amostra) f\u003c-jb.norm.test(amostra) shap[i]\u003c-a$p.value and[i]\u003c-b$p.value cvm[i]\u003c-c$p.value lilli[i]\u003c-d$p.value sf[i]\u003c-e$p.value jb[i]\u003c-f$p.value } power.shapiro\u003c-mean(shap\u003calpha) power.ad\u003c-mean(and\u003calpha) power.cvm\u003c-mean(cvm\u003calpha) power.lillie\u003c-mean(lilli\u003calpha) power.sf\u003c-mean(sf\u003calpha) power.jbr\u003c-mean(jb\u003calpha) poder\u003c-c(power.shapiro,power.ad,power.cvm,power.lillie,power.sf,power.jbr) testedenormalidade\u003c-function(amostra,alpha){ a\u003c-shapiro.test(amostra) b\u003c-ad.test(amostra) c\u003c-cvm.test(amostra) d\u003c-lillie.test(amostra) e\u003c-sf.test(amostra) f\u003c-jb.norm.test(amostra) nomes\u003c-c(a$method,b$method,c$method,d$method,e$method,f$method) pvalores\u003c-c(a$p.value,b$p.value,c$p.value,d$p.value,e$p.value,f$p.value) tamanho\u003c-rep(length(amostra),length(pvalores)) return(data.frame(Testes= nomes ,Pvalor = pvalores, RejeitaH0=ifelse(pvalores\u003c=alpha,'SIM','NAO'),Poder = poder, Tamanho=tamanho)) } save\u003c-testedenormalidade(amostra,alpha) write.csv(save, 'exp10.txt', row.names = FALSE) ########################################################################################################### shap\u003c-NULL and\u003c-NULL cvm\u003c-NULL lilli\u003c-NULL sf\u003c-NULL jb\u003c-NULL for(i in 1:m){ amostra \u003c- rbeta(15,2,2) a\u003c-shapiro.test(amostra) b\u003c-ad.test(amostra) c\u003c-cvm.test(amostra) d\u003c-lillie.test(amostra) e\u003c-sf.test(amostra) f\u003c-jb.norm.test(amostra) shap[i]\u003c-a$p.value and[i]\u003c-b$p.value cvm[i]\u003c-c$p.value lilli[i]\u003c-d$p.value sf[i]\u003c-e$p.value jb[i]\u003c-f$p.value } power.shapiro\u003c-mean(shap\u003calpha) power.ad\u003c-mean(and\u003calpha) power.cvm\u003c-mean(cvm\u003calpha) power.lillie\u003c-mean(lilli\u003calpha) power.sf\u003c-mean(sf\u003calpha) power.jbr\u003c-mean(jb\u003calpha) poder\u003c-c(power.shapiro,power.ad,power.cvm,power.lillie,power.sf,power.jbr) testedenormalidade\u003c-function(amostra,alpha){ a\u003c-shapiro.test(amostra) b\u003c-ad.test(amostra) c\u003c-cvm.test(amostra) d\u003c-lillie.test(amostra) e\u003c-sf.test(amostra) f\u003c-jb.norm.test(amostra) nomes\u003c-c(a$method,b$method,c$method,d$method,e$method,f$method) pvalores\u003c-c(a$p.value,b$p.value,c$p.value,d$p.value,e$p.value,f$p.value) tamanho\u003c-rep(length(amostra),length(pvalores)) return(data.frame(Testes= nomes ,Pvalor = pvalores, RejeitaH0=ifelse(pvalores\u003c=alpha,'SIM','NAO'),Poder = poder, Tamanho=tamanho)) } save\u003c-testedenormalidade(amostra,alpha) write.csv(save, 'exp15.txt', row.names = FALSE) ########################################################################################################### shap\u003c-NULL and\u003c-NULL cvm\u003c-NULL lilli\u003c-NULL sf\u003c-NULL jb\u003c-NULL for(i in 1:m){ amostra \u003c- rbeta(20,2,2) a\u003c-shapiro.test(amostra) b\u003c-ad.test(amostra) c\u003c-cvm.test(amostra) d\u003c-lillie.test(amostra) e\u003c-sf.test(amostra) f\u003c-jb.norm.test(amostra) shap[i]\u003c-a$p.value and[i]\u003c-b$p.value cvm[i]\u003c-c$p.value lilli[i]\u003c-d$p.value sf[i]\u003c-e$p.value jb[i]\u003c-f$p.value } power.shapiro\u003c-mean(shap\u003calpha) power.ad\u003c-mean(and\u003calpha) power.cvm\u003c-mean(cvm\u003calpha) power.lillie\u003c-mean(lilli\u003calpha) power.sf\u003c-mean(sf\u003calpha) power.jbr\u003c-mean(jb\u003calpha) poder\u003c-c(power.shapiro,power.ad,power.cvm,power.lillie,power.sf,power.jbr) testedenormalidade\u003c-function(amostra,alpha){ a\u003c-shapiro.test(amostra) b\u003c-ad.test(amostra) c\u003c-cvm.test(amostra) d\u003c-lillie.test(amostra) e\u003c-sf.test(amostra) f\u003c-jb.norm.test(amostra) nomes\u003c-c(a$method,b$method,c$method,d$method,e$method,f$method) pvalores\u003c-c(a$p.value,b$p.value,c$p.value,d$p.value,e$p.value,f$p.value) tamanho\u003c-rep(length(amostra),length(pvalores)) return(data.frame(Testes= nomes ,Pvalor = pvalores, RejeitaH0=ifelse(pvalores\u003c=alpha,'SIM','NAO'),Poder = poder, Tamanho=tamanho)) } save\u003c-testedenormalidade(amostra,alpha) write.csv(save, 'exp20.txt', row.names = FALSE) ########################################################################################################### shap\u003c-NULL and\u003c-NULL cvm\u003c-NULL lilli\u003c-NULL sf\u003c-NULL jb\u003c-NULL for(i in 1:m){ a","date":"20-02-2019","objectID":"/nove-post/:6:0","tags":null,"title":"Desempenho e concordância entre alguns testes De normalidade sob a presença e ausência de outliers.","uri":"/nove-post/"}]